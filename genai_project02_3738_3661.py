# -*- coding: utf-8 -*-
"""GenAi_Project02_3738_3661.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ReWskG3Lg7gWvkuN6cXJjYW3K8xNHjRe
"""

# ============================================================================
# CELL 1: TASK 1 - PREPROCESSING
# ============================================================================
# Empathetic Conversational Chatbot - Transformer with MultiHead Attention
#
# This cell handles complete data preprocessing including:
# - Loading and exploring the dataset
# - Text normalization (lowercase, clean whitespace, punctuation)
# - Tokenization and vocabulary building from training data only
# - Adding special tokens (<pad>, <bos>, <eos>, <unk>, <sep>)
# - Train/Val/Test split (80/10/10)
# - Saving preprocessed data for next tasks
# ============================================================================

# ----------------------------------------------------------------------------
# STEP 1: Install and Import Required Libraries
# ----------------------------------------------------------------------------
print("=" * 80)
print("INSTALLING REQUIRED LIBRARIES...")
print("=" * 80)

!pip install pandas numpy matplotlib seaborn tqdm -q

import pandas as pd
import numpy as np
import re
import pickle
import json
from collections import Counter, OrderedDict
from typing import List, Dict, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import os
import io

# Set random seed for reproducibility
np.random.seed(42)

print("‚úì All libraries imported successfully!\n")


# ----------------------------------------------------------------------------
# STEP 2: Upload Dataset File
# ----------------------------------------------------------------------------
print("=" * 80)
print("STEP 1: UPLOAD DATASET")
print("=" * 80)
print("Please upload the emotion-emotion_69k.csv file:")
print("Click 'Choose Files' button that appears below and select your CSV file\n")

from google.colab import files
uploaded = files.upload()

# Get the uploaded file name
uploaded_filename = list(uploaded.keys())[0]
DATASET_PATH = uploaded_filename

print("\n" + "=" * 80)
print(f"‚úì File uploaded successfully: {uploaded_filename}")
print(f"‚úì File size: {len(uploaded[uploaded_filename]) / 1024 / 1024:.2f} MB")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 3: Load and Explore the Dataset
# ----------------------------------------------------------------------------
print("=" * 80)
print("STEP 2: LOADING AND EXPLORING DATASET")
print("=" * 80)

# Load the dataset
df = pd.read_csv(DATASET_PATH)

print(f"Total number of rows: {len(df)}")
print(f"Total number of columns: {len(df.columns)}")
print(f"\nColumn names: {df.columns.tolist()}")
print(f"\nFirst few rows:")
print(df.head(3))
print(f"\nMissing values:\n{df.isnull().sum()}")
print(f"\nDataset shape: {df.shape}")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 4: Parse Dialogues to Extract Customer and Agent Text
# ----------------------------------------------------------------------------
print("=" * 80)
print("STEP 3: PARSING DIALOGUES")
print("=" * 80)

def parse_dialogue(dialogue_text, labels_text):
    """
    Parse the empathetic_dialogues and labels columns to extract customer and agent text.

    The dataset structure:
    - empathetic_dialogues: contains "Customer :<customer_text>\nAgent :"
    - labels: contains the agent's reply

    Args:
        dialogue_text: String from empathetic_dialogues column
        labels_text: String from labels column (agent reply)

    Returns:
        tuple: (customer_utterance, agent_reply)
    """
    if pd.isna(dialogue_text) or pd.isna(labels_text):
        return None, None

    # Extract customer utterance from empathetic_dialogues
    # It contains "Customer :<text>\nAgent :"
    customer_utterance = dialogue_text

    # Remove "Customer :" prefix if present
    if "Customer :" in customer_utterance:
        customer_utterance = customer_utterance.split("Customer :")[1]

    # Remove "\nAgent :" or "Agent :" suffix if present
    if "\nAgent :" in customer_utterance:
        customer_utterance = customer_utterance.split("\nAgent :")[0]
    elif "Agent :" in customer_utterance:
        customer_utterance = customer_utterance.split("Agent :")[0]

    customer_utterance = customer_utterance.strip()

    # Agent reply is directly from labels column
    agent_reply = str(labels_text).strip()

    # Only return valid pairs
    if customer_utterance and agent_reply and agent_reply != 'nan':
        return customer_utterance, agent_reply

    return None, None


# Apply parsing to the dataset
parsed_data = []

for idx, row in tqdm(df.iterrows(), total=len(df), desc="Parsing dialogues"):
    customer_utterance, agent_reply = parse_dialogue(
        row['empathetic_dialogues'],
        row['labels']
    )

    # Only keep rows where both customer and agent text exist
    if customer_utterance and agent_reply:
        parsed_data.append({
            'situation': row['Situation'] if pd.notna(row['Situation']) else '',
            'emotion': row['emotion'] if pd.notna(row['emotion']) else '',
            'customer_utterance': customer_utterance,
            'agent_reply': agent_reply
        })

# Create a new clean dataframe
df_clean = pd.DataFrame(parsed_data)

print(f"\n‚úì Total valid dialogues parsed: {len(df_clean)}")
print(f"\nEmotion distribution:")
print(df_clean['emotion'].value_counts())
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 5: Define and Apply Text Normalization
# ----------------------------------------------------------------------------
print("=" * 80)
print("STEP 4: TEXT NORMALIZATION")
print("=" * 80)

def normalize_text(text):
    """
    Normalize text by:
    - Converting to lowercase
    - Normalizing whitespace (replace multiple spaces with single space)
    - Normalizing punctuation (add spaces around punctuation marks)
    - Expanding contractions
    - Removing extra whitespace

    Args:
        text: Input string to normalize

    Returns:
        Normalized string
    """
    if not isinstance(text, str):
        return ""

    # Convert to lowercase
    text = text.lower()

    # Normalize common contractions (helps with consistency)
    contractions = {
        "won't": "will not",
        "can't": "cannot",
        "n't": " not",
        "'re": " are",
        "'ve": " have",
        "'ll": " will",
        "'d": " would",
        "'m": " am"
    }
    for contraction, expansion in contractions.items():
        text = text.replace(contraction, expansion)

    # Add spaces around punctuation marks for better tokenization
    text = re.sub(r'([.!?,;:\"\'])', r' \1 ', text)

    # Replace multiple spaces with a single space
    text = re.sub(r'\s+', ' ', text)

    # Remove leading and trailing whitespace
    text = text.strip()

    return text


# Normalize all text columns
print("Normalizing text fields...")
df_clean['situation_normalized'] = df_clean['situation'].apply(normalize_text)
df_clean['customer_normalized'] = df_clean['customer_utterance'].apply(normalize_text)
df_clean['agent_normalized'] = df_clean['agent_reply'].apply(normalize_text)
df_clean['emotion_normalized'] = df_clean['emotion'].str.lower().str.strip()

print("‚úì Text normalization complete!")
print("\nNormalization examples:")
for i in range(2):
    print(f"\nExample {i+1}:")
    print(f"  Original: {df_clean.iloc[i]['customer_utterance']}")
    print(f"  Normalized: {df_clean.iloc[i]['customer_normalized']}")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 6: Train/Validation/Test Split (80/10/10)
# ----------------------------------------------------------------------------
print("=" * 80)
print("STEP 5: DATASET SPLITTING")
print("=" * 80)

# Shuffle the dataset
df_clean = df_clean.sample(frac=1, random_state=42).reset_index(drop=True)

# Calculate split sizes
total_size = len(df_clean)
train_size = int(0.8 * total_size)
val_size = int(0.1 * total_size)
test_size = total_size - train_size - val_size

# Split the data
train_df = df_clean[:train_size].reset_index(drop=True)
val_df = df_clean[train_size:train_size+val_size].reset_index(drop=True)
test_df = df_clean[train_size+val_size:].reset_index(drop=True)

print(f"Total samples: {total_size}")
print(f"Training samples: {len(train_df)} ({len(train_df)/total_size*100:.1f}%)")
print(f"Validation samples: {len(val_df)} ({len(val_df)/total_size*100:.1f}%)")
print(f"Test samples: {len(test_df)} ({len(test_df)/total_size*100:.1f}%)")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 7: Define Tokenization Function
# ----------------------------------------------------------------------------
def simple_tokenize(text):
    """
    Simple word-level tokenizer that splits text by whitespace.
    The text should already be normalized before tokenization.

    Args:
        text: Input string (should be normalized)

    Returns:
        List of tokens (words)
    """
    if not isinstance(text, str):
        return []

    # Split by whitespace
    tokens = text.split()

    return tokens


# ----------------------------------------------------------------------------
# STEP 8: Build Vocabulary from Training Data ONLY
# ----------------------------------------------------------------------------
print("=" * 80)
print("STEP 6: BUILDING VOCABULARY (from training data only)")
print("=" * 80)

def build_vocabulary(dataframe, min_freq=2):
    """
    Build vocabulary from the training dataset.

    Args:
        dataframe: Training dataframe with normalized text
        min_freq: Minimum frequency for a word to be included in vocabulary

    Returns:
        Dictionary mapping tokens to IDs
    """
    # Counter to count word frequencies
    word_counter = Counter()

    # Count words from all text fields in training data
    print("Counting word frequencies in training data...")

    for idx, row in tqdm(dataframe.iterrows(), total=len(dataframe), desc="Building vocab"):
        # Tokenize and count from all fields
        situation_tokens = simple_tokenize(row['situation_normalized'])
        word_counter.update(situation_tokens)

        customer_tokens = simple_tokenize(row['customer_normalized'])
        word_counter.update(customer_tokens)

        agent_tokens = simple_tokenize(row['agent_normalized'])
        word_counter.update(agent_tokens)

    print(f"\nTotal unique words found: {len(word_counter)}")
    print(f"Total word occurrences: {sum(word_counter.values())}")

    # Filter words by minimum frequency
    filtered_words = [word for word, freq in word_counter.items() if freq >= min_freq]
    print(f"Words with frequency >= {min_freq}: {len(filtered_words)}")

    # Define special tokens (these are crucial for the model)
    special_tokens = [
        '<pad>',  # Padding token - for making all sequences same length
        '<unk>',  # Unknown token - for words not in vocabulary
        '<bos>',  # Beginning of sequence - marks start of target
        '<eos>',  # End of sequence - marks end of target
        '<sep>',  # Separator token - separates input components
    ]

    # Build vocabulary: token -> id mapping
    vocab = {}

    # Add special tokens first (they get the lowest IDs: 0, 1, 2, 3, 4)
    for idx, token in enumerate(special_tokens):
        vocab[token] = idx

    # Add regular words (sorted alphabetically for consistency)
    for idx, word in enumerate(sorted(filtered_words), start=len(special_tokens)):
        vocab[word] = idx

    print(f"\n‚úì Vocabulary built successfully!")
    print(f"Final vocabulary size: {len(vocab)}")

    return vocab, word_counter


# Build vocabulary from TRAINING data only (important: not from val/test!)
MIN_FREQUENCY = 2  # Words must appear at least 2 times

vocab, word_freq = build_vocabulary(train_df, min_freq=MIN_FREQUENCY)

# Create reverse vocabulary (id -> token) for decoding later
id_to_token = {idx: token for token, idx in vocab.items()}

print("\nSpecial tokens and their IDs:")
special_tokens_list = ['<pad>', '<unk>', '<bos>', '<eos>', '<sep>']
for token in special_tokens_list:
    print(f"  {token}: {vocab[token]}")

print(f"\nMost common words in training data (top 15):")
for word, freq in word_freq.most_common(15):
    print(f"  '{word}': {freq} occurrences")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 9: Token-ID Conversion Functions
# ----------------------------------------------------------------------------
def tokens_to_ids(tokens, vocab):
    """
    Convert list of tokens to list of IDs using vocabulary.
    Unknown tokens are mapped to <unk> token ID.

    Args:
        tokens: List of token strings
        vocab: Vocabulary dictionary (token -> id)

    Returns:
        List of token IDs
    """
    unk_id = vocab['<unk>']
    return [vocab.get(token, unk_id) for token in tokens]


def ids_to_tokens(ids, id_to_token):
    """
    Convert list of IDs to list of tokens.

    Args:
        ids: List of token IDs
        id_to_token: Reverse vocabulary dictionary (id -> token)

    Returns:
        List of tokens
    """
    return [id_to_token.get(idx, '<unk>') for idx in ids]


# ----------------------------------------------------------------------------
# STEP 10: Create Input-Output Pairs with Special Tokens
# ----------------------------------------------------------------------------
print("=" * 80)
print("STEP 7: PREPARING INPUT-OUTPUT PAIRS")
print("=" * 80)

def prepare_input_output(row, vocab):
    """
    Prepare input and output sequences with special tokens.

    Input format (X):
        emotion: {emotion} <sep> situation: {situation} <sep> customer: {customer} <sep>

    Target format (Y):
        <bos> {agent_reply} <eos>

    Args:
        row: DataFrame row containing normalized text
        vocab: Vocabulary dictionary

    Returns:
        tuple: (input_ids, target_ids)
    """
    # Prepare input text with structure
    input_text = f"emotion : {row['emotion_normalized']} <sep> situation : {row['situation_normalized']} <sep> customer : {row['customer_normalized']} <sep>"

    # Tokenize input (handle <sep> token specially)
    input_tokens = []
    for token in input_text.split():
        if token == '<sep>':
            input_tokens.append('<sep>')
        else:
            input_tokens.append(token)

    # Convert input tokens to IDs
    input_ids = tokens_to_ids(input_tokens, vocab)

    # Prepare target text with <bos> and <eos> markers
    target_text = row['agent_normalized']
    target_tokens = ['<bos>'] + simple_tokenize(target_text) + ['<eos>']

    # Convert target tokens to IDs
    target_ids = tokens_to_ids(target_tokens, vocab)

    return input_ids, target_ids


# Test on sample examples
print("Input-Output format examples:\n")
for i in range(2):
    row = train_df.iloc[i]
    input_ids, target_ids = prepare_input_output(row, vocab)

    print(f"Example {i+1}:")
    print(f"  Emotion: {row['emotion_normalized']}")
    print(f"  Situation: {row['situation_normalized'][:60]}...")
    print(f"  Customer: {row['customer_normalized']}")
    print(f"  Agent: {row['agent_normalized']}")
    print(f"  Input length: {len(input_ids)} tokens")
    print(f"  Target length: {len(target_ids)} tokens")
    print(f"  Target tokens: {ids_to_tokens(target_ids, id_to_token)}")
    print()

print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 11: Process All Datasets (Train/Val/Test)
# ----------------------------------------------------------------------------
print("=" * 80)
print("STEP 8: PROCESSING ALL DATASETS")
print("=" * 80)

def process_dataset(dataframe, vocab, split_name):
    """
    Process entire dataset to create input-output pairs.

    Args:
        dataframe: DataFrame to process
        vocab: Vocabulary dictionary
        split_name: Name of the split (for logging)

    Returns:
        List of dictionaries containing input_ids and target_ids
    """
    processed_data = []

    for idx, row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=f"Processing {split_name}"):
        input_ids, target_ids = prepare_input_output(row, vocab)

        processed_data.append({
            'input_ids': input_ids,
            'target_ids': target_ids,
            'emotion': row['emotion_normalized'],
            'input_length': len(input_ids),
            'target_length': len(target_ids)
        })

    return processed_data


# Process all three splits
train_processed = process_dataset(train_df, vocab, "Training")
val_processed = process_dataset(val_df, vocab, "Validation")
test_processed = process_dataset(test_df, vocab, "Test")

print(f"\n‚úì Training samples processed: {len(train_processed)}")
print(f"‚úì Validation samples processed: {len(val_processed)}")
print(f"‚úì Test samples processed: {len(test_processed)}")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 12: Analyze Sequence Lengths
# ----------------------------------------------------------------------------
print("=" * 80)
print("STEP 9: ANALYZING SEQUENCE LENGTHS")
print("=" * 80)

# Extract sequence lengths from training data
train_input_lengths = [sample['input_length'] for sample in train_processed]
train_target_lengths = [sample['target_length'] for sample in train_processed]

# Calculate statistics
print("\nInput sequences (X) statistics:")
print(f"  Mean length: {np.mean(train_input_lengths):.2f}")
print(f"  Median length: {np.median(train_input_lengths):.2f}")
print(f"  Min length: {np.min(train_input_lengths)}")
print(f"  Max length: {np.max(train_input_lengths)}")
print(f"  95th percentile: {np.percentile(train_input_lengths, 95):.2f}")
print(f"  99th percentile: {np.percentile(train_input_lengths, 99):.2f}")

print("\nTarget sequences (Y) statistics:")
print(f"  Mean length: {np.mean(train_target_lengths):.2f}")
print(f"  Median length: {np.median(train_target_lengths):.2f}")
print(f"  Min length: {np.min(train_target_lengths)}")
print(f"  Max length: {np.max(train_target_lengths)}")
print(f"  95th percentile: {np.percentile(train_target_lengths, 95):.2f}")
print(f"  99th percentile: {np.percentile(train_target_lengths, 99):.2f}")

# Suggest max lengths based on 95th percentile (captures most data while avoiding outliers)
suggested_max_input_len = int(np.percentile(train_input_lengths, 95))
suggested_max_target_len = int(np.percentile(train_target_lengths, 95))

print(f"\nüí° SUGGESTED MAX LENGTHS (based on 95th percentile):")
print(f"  Max input length: {suggested_max_input_len}")
print(f"  Max target length: {suggested_max_target_len}")
print("  (These will be used for padding in the model)")

# Visualize distributions
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].hist(train_input_lengths, bins=50, edgecolor='black', alpha=0.7, color='steelblue')
axes[0].axvline(suggested_max_input_len, color='red', linestyle='--', linewidth=2,
                label=f'95th percentile: {suggested_max_input_len}')
axes[0].set_xlabel('Sequence Length', fontsize=12)
axes[0].set_ylabel('Frequency', fontsize=12)
axes[0].set_title('Input Sequence Length Distribution', fontsize=14, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

axes[1].hist(train_target_lengths, bins=50, edgecolor='black', alpha=0.7, color='forestgreen')
axes[1].axvline(suggested_max_target_len, color='red', linestyle='--', linewidth=2,
                label=f'95th percentile: {suggested_max_target_len}')
axes[1].set_xlabel('Sequence Length', fontsize=12)
axes[1].set_ylabel('Frequency', fontsize=12)
axes[1].set_title('Target Sequence Length Distribution', fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 13: Save Preprocessed Data and Vocabulary
# ----------------------------------------------------------------------------
print("=" * 80)
print("STEP 10: SAVING PREPROCESSED DATA")
print("=" * 80)

# Create directory to save all preprocessed data
save_dir = '/content/preprocessed_data'
os.makedirs(save_dir, exist_ok=True)

# Save vocabulary (token -> id mapping)
with open(f'{save_dir}/vocab.json', 'w') as f:
    json.dump(vocab, f, indent=2)

# Save reverse vocabulary (id -> token mapping)
with open(f'{save_dir}/id_to_token.json', 'w') as f:
    json.dump(id_to_token, f, indent=2)

print("‚úì Vocabulary saved!")

# Save processed datasets (in pickle format for efficiency)
with open(f'{save_dir}/train_processed.pkl', 'wb') as f:
    pickle.dump(train_processed, f)

with open(f'{save_dir}/val_processed.pkl', 'wb') as f:
    pickle.dump(val_processed, f)

with open(f'{save_dir}/test_processed.pkl', 'wb') as f:
    pickle.dump(test_processed, f)

print("‚úì Processed datasets saved!")

# Save original dataframes (for reference and debugging)
train_df.to_csv(f'{save_dir}/train_df.csv', index=False)
val_df.to_csv(f'{save_dir}/val_df.csv', index=False)
test_df.to_csv(f'{save_dir}/test_df.csv', index=False)

print("‚úì Original dataframes saved!")

# Save configuration file with all important parameters
config = {
    'vocab_size': len(vocab),
    'min_frequency': MIN_FREQUENCY,
    'train_size': len(train_processed),
    'val_size': len(val_processed),
    'test_size': len(test_processed),
    'suggested_max_input_len': suggested_max_input_len,
    'suggested_max_target_len': suggested_max_target_len,
    'special_tokens': {
        'pad_token': '<pad>',
        'unk_token': '<unk>',
        'bos_token': '<bos>',
        'eos_token': '<eos>',
        'sep_token': '<sep>',
        'pad_id': vocab['<pad>'],
        'unk_id': vocab['<unk>'],
        'bos_id': vocab['<bos>'],
        'eos_id': vocab['<eos>'],
        'sep_id': vocab['<sep>']
    },
    'input_format': 'emotion: {emotion} <sep> situation: {situation} <sep> customer: {customer} <sep>',
    'target_format': '<bos> {agent_reply} <eos>'
}

with open(f'{save_dir}/config.json', 'w') as f:
    json.dump(config, f, indent=2)

print("‚úì Configuration saved!")

print(f"\nAll files saved to: {save_dir}")
print("\nSaved files:")
print("  üìÑ vocab.json - Vocabulary (token -> id)")
print("  üìÑ id_to_token.json - Reverse vocabulary (id -> token)")
print("  üìÑ train_processed.pkl - Processed training data")
print("  üìÑ val_processed.pkl - Processed validation data")
print("  üìÑ test_processed.pkl - Processed test data")
print("  üìÑ train_df.csv - Original training dataframe")
print("  üìÑ val_df.csv - Original validation dataframe")
print("  üìÑ test_df.csv - Original test dataframe")
print("  üìÑ config.json - All configuration parameters")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# FINAL SUMMARY
# ----------------------------------------------------------------------------
print("=" * 80)
print("‚úÖ TASK 1 COMPLETE - PREPROCESSING SUMMARY")
print("=" * 80)

print("\nüìä DATASET STATISTICS:")
print(f"   ‚Ä¢ Total samples: {len(df_clean)}")
print(f"   ‚Ä¢ Training: {len(train_processed)} samples (80%)")
print(f"   ‚Ä¢ Validation: {len(val_processed)} samples (10%)")
print(f"   ‚Ä¢ Test: {len(test_processed)} samples (10%)")

print("\nüìö VOCABULARY:")
print(f"   ‚Ä¢ Vocabulary size: {len(vocab)}")
print(f"   ‚Ä¢ Special tokens: <pad>, <unk>, <bos>, <eos>, <sep>")
print(f"   ‚Ä¢ Minimum word frequency: {MIN_FREQUENCY}")

print("\nüìè SEQUENCE LENGTHS:")
print(f"   ‚Ä¢ Suggested max input length: {suggested_max_input_len}")
print(f"   ‚Ä¢ Suggested max target length: {suggested_max_target_len}")

print("\nüî§ TEXT NORMALIZATION:")
print("   ‚úì Converted to lowercase")
print("   ‚úì Normalized whitespace")
print("   ‚úì Normalized punctuation")
print("   ‚úì Expanded contractions")

print("\nüìù DATA FORMAT:")
print("   ‚Ä¢ Input (X): emotion: {emotion} <sep> situation: {situation} <sep> customer: {customer} <sep>")
print("   ‚Ä¢ Target (Y): <bos> {agent_reply} <eos>")

print("\n" + "=" * 80)
print("üéâ ALL PREPROCESSING COMPLETE!")
print("=" * 80)
print("‚úÖ Data is ready for Task 2: Model Architecture (Transformer)")
print("=" * 80)

# Quick verification - test loading saved files
print("\nüîç Quick verification - testing saved files...")
with open(f'{save_dir}/vocab.json', 'r') as f:
    loaded_vocab = json.load(f)
print(f"‚úì Successfully loaded vocabulary: {len(loaded_vocab)} tokens")

with open(f'{save_dir}/train_processed.pkl', 'rb') as f:
    loaded_train = pickle.load(f)
print(f"‚úì Successfully loaded training data: {len(loaded_train)} samples")

print("\n" + "=" * 80)
print("All files saved and verified successfully!")
print("You are now ready to proceed to Task 2!")
print("=" * 80)

from google.colab import drive
drive.mount('/content/drive')

# ============================================================================
# CELL 2: TASK 2 - MODEL ARCHITECTURE (Transformer from Scratch)
# ============================================================================
# Empathetic Conversational Chatbot - Transformer with MultiHead Attention
#
# This cell implements a complete Transformer encoder-decoder model from scratch:
# - Positional Encoding (for position information in sequences)
# - Multi-Head Attention (core attention mechanism)
# - Feed-Forward Networks (position-wise transformation)
# - Encoder Layer (self-attention + feed-forward)
# - Decoder Layer (self-attention + cross-attention + feed-forward)
# - Complete Transformer Model (encoder + decoder)
#
# All components built from scratch using PyTorch (no pretrained weights)
# ============================================================================

# ----------------------------------------------------------------------------
# STEP 1: Install and Import Required Libraries
# ----------------------------------------------------------------------------
print("=" * 80)
print("INSTALLING PYTORCH AND REQUIRED LIBRARIES...")
print("=" * 80)

!pip install torch torchvision torchaudio -q

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
import json
import pickle

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\n‚úì PyTorch version: {torch.__version__}")
print(f"‚úì Device: {device}")
if torch.cuda.is_available():
    print(f"‚úì GPU: {torch.cuda.get_device_name(0)}")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 2: Load Preprocessed Data and Configuration
# ----------------------------------------------------------------------------
print("=" * 80)
print("LOADING PREPROCESSED DATA")
print("=" * 80)

# Load configuration from Task 1
with open('/content/preprocessed_data/config.json', 'r') as f:
    config = json.load(f)

# Load vocabulary
with open('/content/preprocessed_data/vocab.json', 'r') as f:
    vocab = json.load(f)

with open('/content/preprocessed_data/id_to_token.json', 'r') as f:
    id_to_token = json.load(f)
    # Convert string keys back to integers
    id_to_token = {int(k): v for k, v in id_to_token.items()}

print(f"‚úì Vocabulary size: {config['vocab_size']}")
print(f"‚úì Training samples: {config['train_size']}")
print(f"‚úì Validation samples: {config['val_size']}")
print(f"‚úì Test samples: {config['test_size']}")
print(f"‚úì Max input length: {config['suggested_max_input_len']}")
print(f"‚úì Max target length: {config['suggested_max_target_len']}")
print("\nSpecial token IDs:")
for token_name, token_id in config['special_tokens'].items():
    if token_name.endswith('_id'):
        print(f"  {token_name}: {token_id}")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 3: Define Model Hyperparameters
# ----------------------------------------------------------------------------
print("=" * 80)
print("MODEL HYPERPARAMETERS")
print("=" * 80)

# Model architecture hyperparameters (as per task requirements)
MODEL_CONFIG = {
    # Vocabulary and special tokens
    'vocab_size': config['vocab_size'],
    'pad_token_id': config['special_tokens']['pad_id'],
    'bos_token_id': config['special_tokens']['bos_id'],
    'eos_token_id': config['special_tokens']['eos_id'],

    # Sequence lengths
    'max_input_len': config['suggested_max_input_len'],    # 79
    'max_target_len': config['suggested_max_target_len'],  # 34

    # Model dimensions
    'd_model': 512,              # Embedding dimension (256 or 512 as suggested)
    'n_heads': 8,                # Number of attention heads (2 as minimum, using 8 for better performance)
    'd_ff': 2048,                # Feed-forward dimension (typically 4 * d_model)

    # Number of layers
    'n_encoder_layers': 2,       # Number of encoder layers
    'n_decoder_layers': 2,       # Number of decoder layers

    # Regularization
    'dropout': 0.1,              # Dropout rate (0.1-0.3 as suggested)

    # Training
    'device': device
}

print("Model Configuration:")
for key, value in MODEL_CONFIG.items():
    print(f"  {key}: {value}")
print("=" * 80 + "\n")


# ============================================================================
# COMPONENT 1: POSITIONAL ENCODING
# ============================================================================
# Positional Encoding adds position information to embeddings
# since Transformers don't have inherent position awareness like RNNs
# Uses sine and cosine functions of different frequencies
# ============================================================================

class PositionalEncoding(nn.Module):
    """
    Implements positional encoding to inject position information into embeddings.

    Formula:
        PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
        PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

    where:
        - pos = position in sequence
        - i = dimension index
        - d_model = embedding dimension
    """

    def __init__(self, d_model, max_len=5000, dropout=0.1):
        """
        Args:
            d_model: Embedding dimension
            max_len: Maximum sequence length
            dropout: Dropout probability
        """
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Create a matrix of shape (max_len, d_model) for positional encodings
        pe = torch.zeros(max_len, d_model)

        # Create position indices [0, 1, 2, ..., max_len-1]
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

        # Create the division term for the formula
        # This creates [10000^(0/d_model), 10000^(2/d_model), ..., 10000^(d_model/d_model)]
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                            (-math.log(10000.0) / d_model))

        # Apply sine to even indices (0, 2, 4, ...)
        pe[:, 0::2] = torch.sin(position * div_term)

        # Apply cosine to odd indices (1, 3, 5, ...)
        pe[:, 1::2] = torch.cos(position * div_term)

        # Add batch dimension: (max_len, d_model) -> (1, max_len, d_model)
        pe = pe.unsqueeze(0)

        # Register as buffer (not a parameter, but should be saved with model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Add positional encoding to input embeddings.

        Args:
            x: Input tensor of shape (batch_size, seq_len, d_model)

        Returns:
            Tensor with positional encoding added, same shape as input
        """
        # Add positional encoding to embeddings
        # x.size(1) is the sequence length
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


# ============================================================================
# COMPONENT 2: MULTI-HEAD ATTENTION
# ============================================================================
# Multi-Head Attention allows the model to attend to different parts
# of the sequence from different representation subspaces
# ============================================================================

class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attention mechanism.

    Steps:
    1. Project Q, K, V into multiple heads
    2. Compute scaled dot-product attention for each head
    3. Concatenate all heads
    4. Project back to d_model dimension

    Attention formula:
        Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V
    """

    def __init__(self, d_model, n_heads, dropout=0.1):
        """
        Args:
            d_model: Model dimension (must be divisible by n_heads)
            n_heads: Number of attention heads
            dropout: Dropout probability
        """
        super(MultiHeadAttention, self).__init__()

        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"

        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads  # Dimension per head

        # Linear projections for Q, K, V (one for each)
        self.W_q = nn.Linear(d_model, d_model)  # Query projection
        self.W_k = nn.Linear(d_model, d_model)  # Key projection
        self.W_v = nn.Linear(d_model, d_model)  # Value projection

        # Output projection
        self.W_o = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)

    def split_heads(self, x, batch_size):
        """
        Split the last dimension into (n_heads, d_k).
        Reshape from (batch_size, seq_len, d_model)
               to (batch_size, n_heads, seq_len, d_k)
        """
        x = x.view(batch_size, -1, self.n_heads, self.d_k)
        return x.transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """
        Compute scaled dot-product attention.

        Args:
            Q: Query tensor (batch_size, n_heads, seq_len_q, d_k)
            K: Key tensor (batch_size, n_heads, seq_len_k, d_k)
            V: Value tensor (batch_size, n_heads, seq_len_v, d_k)
            mask: Optional mask tensor

        Returns:
            Attention output and attention weights
        """
        # Compute attention scores: Q * K^T / sqrt(d_k)
        # Shape: (batch_size, n_heads, seq_len_q, seq_len_k)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        # Apply mask (if provided)
        # Mask is used to prevent attention to certain positions
        # (e.g., padding tokens, future tokens in decoder)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Apply softmax to get attention weights
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        # Apply attention weights to values
        # Shape: (batch_size, n_heads, seq_len_q, d_k)
        output = torch.matmul(attention_weights, V)

        return output, attention_weights

    def forward(self, query, key, value, mask=None):
        """
        Forward pass of multi-head attention.

        Args:
            query: Query tensor (batch_size, seq_len_q, d_model)
            key: Key tensor (batch_size, seq_len_k, d_model)
            value: Value tensor (batch_size, seq_len_v, d_model)
            mask: Optional mask tensor

        Returns:
            Output tensor (batch_size, seq_len_q, d_model)
        """
        batch_size = query.size(0)

        # 1. Linear projections
        Q = self.W_q(query)  # (batch_size, seq_len_q, d_model)
        K = self.W_k(key)    # (batch_size, seq_len_k, d_model)
        V = self.W_v(value)  # (batch_size, seq_len_v, d_model)

        # 2. Split into multiple heads
        Q = self.split_heads(Q, batch_size)  # (batch_size, n_heads, seq_len_q, d_k)
        K = self.split_heads(K, batch_size)  # (batch_size, n_heads, seq_len_k, d_k)
        V = self.split_heads(V, batch_size)  # (batch_size, n_heads, seq_len_v, d_k)

        # 3. Apply scaled dot-product attention
        attn_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)

        # 4. Concatenate heads
        # (batch_size, n_heads, seq_len_q, d_k) -> (batch_size, seq_len_q, n_heads, d_k)
        attn_output = attn_output.transpose(1, 2).contiguous()

        # Reshape to (batch_size, seq_len_q, d_model)
        attn_output = attn_output.view(batch_size, -1, self.d_model)

        # 5. Final linear projection
        output = self.W_o(attn_output)

        return output, attention_weights


# ============================================================================
# COMPONENT 3: FEED-FORWARD NETWORK
# ============================================================================
# Position-wise Feed-Forward Network
# Applied to each position separately and identically
# Consists of two linear transformations with ReLU activation
# ============================================================================

class PositionWiseFeedForward(nn.Module):
    """
    Position-wise Feed-Forward Network.

    Architecture:
        FFN(x) = max(0, x * W1 + b1) * W2 + b2

    Or simply:
        Linear -> ReLU -> Dropout -> Linear
    """

    def __init__(self, d_model, d_ff, dropout=0.1):
        """
        Args:
            d_model: Model dimension
            d_ff: Feed-forward dimension (typically 4 * d_model)
            dropout: Dropout probability
        """
        super(PositionWiseFeedForward, self).__init__()

        # First linear layer: d_model -> d_ff
        self.linear1 = nn.Linear(d_model, d_ff)

        # Second linear layer: d_ff -> d_model
        self.linear2 = nn.Linear(d_ff, d_model)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        """
        Args:
            x: Input tensor (batch_size, seq_len, d_model)

        Returns:
            Output tensor (batch_size, seq_len, d_model)
        """
        # Apply first linear transformation and ReLU
        x = F.relu(self.linear1(x))

        # Apply dropout
        x = self.dropout(x)

        # Apply second linear transformation
        x = self.linear2(x)

        return x


# ============================================================================
# COMPONENT 4: ENCODER LAYER
# ============================================================================
# Single Encoder Layer consisting of:
# 1. Multi-Head Self-Attention
# 2. Add & Norm (residual connection + layer normalization)
# 3. Feed-Forward Network
# 4. Add & Norm (residual connection + layer normalization)
# ============================================================================

class EncoderLayer(nn.Module):
    """
    Single Transformer Encoder Layer.

    Architecture:
        x -> Self-Attention -> Add & Norm -> Feed-Forward -> Add & Norm -> output
    """

    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        """
        Args:
            d_model: Model dimension
            n_heads: Number of attention heads
            d_ff: Feed-forward dimension
            dropout: Dropout probability
        """
        super(EncoderLayer, self).__init__()

        # Multi-head self-attention
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)

        # Feed-forward network
        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)

        # Layer normalization (applied before attention and FFN - Pre-LN variant)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        # Dropout
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        Args:
            x: Input tensor (batch_size, seq_len, d_model)
            mask: Optional attention mask

        Returns:
            Output tensor (batch_size, seq_len, d_model)
        """
        # Self-attention with residual connection and layer norm
        # Pre-LN: Norm -> Attention -> Residual
        attn_output, _ = self.self_attention(x, x, x, mask)
        x = x + self.dropout(attn_output)  # Residual connection
        x = self.norm1(x)  # Layer normalization

        # Feed-forward with residual connection and layer norm
        ff_output = self.feed_forward(x)
        x = x + self.dropout(ff_output)  # Residual connection
        x = self.norm2(x)  # Layer normalization

        return x


# ============================================================================
# COMPONENT 5: DECODER LAYER
# ============================================================================
# Single Decoder Layer consisting of:
# 1. Masked Multi-Head Self-Attention (looks only at previous positions)
# 2. Add & Norm
# 3. Multi-Head Cross-Attention (attends to encoder output)
# 4. Add & Norm
# 5. Feed-Forward Network
# 6. Add & Norm
# ============================================================================

class DecoderLayer(nn.Module):
    """
    Single Transformer Decoder Layer.

    Architecture:
        x -> Masked Self-Attention -> Add & Norm
          -> Cross-Attention (with encoder) -> Add & Norm
          -> Feed-Forward -> Add & Norm -> output
    """

    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        """
        Args:
            d_model: Model dimension
            n_heads: Number of attention heads
            d_ff: Feed-forward dimension
            dropout: Dropout probability
        """
        super(DecoderLayer, self).__init__()

        # Masked multi-head self-attention
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)

        # Multi-head cross-attention (attends to encoder output)
        self.cross_attention = MultiHeadAttention(d_model, n_heads, dropout)

        # Feed-forward network
        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)

        # Layer normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        # Dropout
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        """
        Args:
            x: Decoder input (batch_size, tgt_seq_len, d_model)
            encoder_output: Encoder output (batch_size, src_seq_len, d_model)
            src_mask: Source (encoder) mask
            tgt_mask: Target (decoder) mask (prevents looking at future tokens)

        Returns:
            Output tensor (batch_size, tgt_seq_len, d_model)
        """
        # 1. Masked self-attention (decoder attends to its own previous outputs)
        self_attn_output, _ = self.self_attention(x, x, x, tgt_mask)
        x = x + self.dropout(self_attn_output)  # Residual connection
        x = self.norm1(x)  # Layer normalization

        # 2. Cross-attention (decoder attends to encoder output)
        # Query from decoder, Key and Value from encoder
        cross_attn_output, _ = self.cross_attention(x, encoder_output, encoder_output, src_mask)
        x = x + self.dropout(cross_attn_output)  # Residual connection
        x = self.norm2(x)  # Layer normalization

        # 3. Feed-forward network
        ff_output = self.feed_forward(x)
        x = x + self.dropout(ff_output)  # Residual connection
        x = self.norm3(x)  # Layer normalization

        return x


# ============================================================================
# COMPONENT 6: COMPLETE TRANSFORMER MODEL
# ============================================================================
# Full Transformer encoder-decoder architecture
# Combines all components into a complete model
# ============================================================================

class Transformer(nn.Module):
    """
    Complete Transformer Model for sequence-to-sequence tasks.

    Architecture:
        Input -> Embedding + Positional Encoding -> Encoder
              -> Decoder <- Target Embedding + Positional Encoding
              -> Linear -> Output
    """

    def __init__(self, config):
        """
        Args:
            config: Dictionary with model configuration
        """
        super(Transformer, self).__init__()

        # Store configuration
        self.config = config
        self.vocab_size = config['vocab_size']
        self.d_model = config['d_model']
        self.pad_token_id = config['pad_token_id']

        # Embedding layers (shared between encoder and decoder for efficiency)
        # We use the same embedding for both source and target
        self.embedding = nn.Embedding(self.vocab_size, self.d_model,
                                     padding_idx=self.pad_token_id)

        # Positional encoding
        max_len = max(config['max_input_len'], config['max_target_len']) + 10
        self.positional_encoding = PositionalEncoding(
            self.d_model,
            max_len=max_len,
            dropout=config['dropout']
        )

        # Encoder layers (stack of N encoder layers)
        self.encoder_layers = nn.ModuleList([
            EncoderLayer(
                config['d_model'],
                config['n_heads'],
                config['d_ff'],
                config['dropout']
            ) for _ in range(config['n_encoder_layers'])
        ])

        # Decoder layers (stack of N decoder layers)
        self.decoder_layers = nn.ModuleList([
            DecoderLayer(
                config['d_model'],
                config['n_heads'],
                config['d_ff'],
                config['dropout']
            ) for _ in range(config['n_decoder_layers'])
        ])

        # Final linear layer to project to vocabulary size
        self.output_projection = nn.Linear(self.d_model, self.vocab_size)

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        """
        Initialize model weights.
        Uses Xavier uniform initialization for better gradient flow.
        """
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def create_padding_mask(self, seq):
        """
        Create mask for padding tokens.

        Args:
            seq: Input sequence (batch_size, seq_len)

        Returns:
            Mask tensor where 1 = valid token, 0 = padding token
        """
        # Create mask: 1 for non-padding tokens, 0 for padding tokens
        mask = (seq != self.pad_token_id).unsqueeze(1).unsqueeze(2)
        return mask  # (batch_size, 1, 1, seq_len)

    def create_look_ahead_mask(self, size):
        """
        Create mask to prevent decoder from looking at future tokens.
        This is crucial for autoregressive generation.

        Args:
            size: Sequence length

        Returns:
            Lower triangular mask
        """
        mask = torch.tril(torch.ones(size, size)).unsqueeze(0).unsqueeze(0)
        return mask  # (1, 1, size, size)

    def encode(self, src, src_mask=None):
        """
        Encode source sequence.

        Args:
            src: Source sequence (batch_size, src_seq_len)
            src_mask: Source mask

        Returns:
            Encoder output (batch_size, src_seq_len, d_model)
        """
        # Embed source tokens and add positional encoding
        x = self.embedding(src) * math.sqrt(self.d_model)
        x = self.positional_encoding(x)

        # Pass through encoder layers
        for encoder_layer in self.encoder_layers:
            x = encoder_layer(x, src_mask)

        return x

    def decode(self, tgt, encoder_output, src_mask=None, tgt_mask=None):
        """
        Decode target sequence.

        Args:
            tgt: Target sequence (batch_size, tgt_seq_len)
            encoder_output: Output from encoder
            src_mask: Source mask
            tgt_mask: Target mask

        Returns:
            Decoder output (batch_size, tgt_seq_len, d_model)
        """
        # Embed target tokens and add positional encoding
        x = self.embedding(tgt) * math.sqrt(self.d_model)
        x = self.positional_encoding(x)

        # Pass through decoder layers
        for decoder_layer in self.decoder_layers:
            x = decoder_layer(x, encoder_output, src_mask, tgt_mask)

        return x

    def forward(self, src, tgt):
        """
        Forward pass of the Transformer.

        Args:
            src: Source sequence (batch_size, src_seq_len)
            tgt: Target sequence (batch_size, tgt_seq_len)

        Returns:
            Output logits (batch_size, tgt_seq_len, vocab_size)
        """
        # Create masks
        src_mask = self.create_padding_mask(src)

        # For target, we need both padding mask and look-ahead mask
        # We combine them using element-wise multiplication (both are 0/1 masks)
        tgt_padding_mask = self.create_padding_mask(tgt)
        tgt_look_ahead_mask = self.create_look_ahead_mask(tgt.size(1)).to(tgt.device)
        tgt_mask = tgt_padding_mask * tgt_look_ahead_mask  # Element-wise multiplication for combining masks

        # Encode source
        encoder_output = self.encode(src, src_mask)

        # Decode target
        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)

        # Project to vocabulary size
        output = self.output_projection(decoder_output)

        return output


# ----------------------------------------------------------------------------
# STEP 4: Initialize the Model
# ----------------------------------------------------------------------------
print("=" * 80)
print("INITIALIZING TRANSFORMER MODEL")
print("=" * 80)

# Create the model
model = Transformer(MODEL_CONFIG).to(device)

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\n‚úì Model created successfully!")
print(f"\nModel Statistics:")
print(f"  Total parameters: {total_params:,}")
print(f"  Trainable parameters: {trainable_params:,}")
print(f"  Model size: ~{total_params * 4 / 1024 / 1024:.2f} MB (float32)")

print(f"\nModel Architecture:")
print(f"  Encoder layers: {MODEL_CONFIG['n_encoder_layers']}")
print(f"  Decoder layers: {MODEL_CONFIG['n_decoder_layers']}")
print(f"  Attention heads: {MODEL_CONFIG['n_heads']}")
print(f"  Embedding dimension: {MODEL_CONFIG['d_model']}")
print(f"  Feed-forward dimension: {MODEL_CONFIG['d_ff']}")
print(f"  Dropout: {MODEL_CONFIG['dropout']}")

print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 5: Test the Model with Sample Data
# ----------------------------------------------------------------------------
print("=" * 80)
print("TESTING MODEL WITH SAMPLE DATA")
print("=" * 80)

# Create sample batch
batch_size = 4
src_seq_len = 20
tgt_seq_len = 10

# Random input (simulating tokenized sequences)
sample_src = torch.randint(0, MODEL_CONFIG['vocab_size'], (batch_size, src_seq_len)).to(device)
sample_tgt = torch.randint(0, MODEL_CONFIG['vocab_size'], (batch_size, tgt_seq_len)).to(device)

print(f"Sample input shapes:")
print(f"  Source: {sample_src.shape}")
print(f"  Target: {sample_tgt.shape}")

# Forward pass
with torch.no_grad():
    output = model(sample_src, sample_tgt)

print(f"\nModel output shape: {output.shape}")
print(f"Expected shape: (batch_size={batch_size}, tgt_seq_len={tgt_seq_len}, vocab_size={MODEL_CONFIG['vocab_size']})")

# Verify output shape
assert output.shape == (batch_size, tgt_seq_len, MODEL_CONFIG['vocab_size']), "Output shape mismatch!"

print("\n‚úì Model test passed successfully!")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 6: Display Model Summary
# ----------------------------------------------------------------------------
print("=" * 80)
print("DETAILED MODEL SUMMARY")
print("=" * 80)

print("\nModel Components:")
print("  1. Embedding Layer")
print(f"     - Input: Token IDs (vocab_size={MODEL_CONFIG['vocab_size']})")
print(f"     - Output: Embeddings (d_model={MODEL_CONFIG['d_model']})")

print("\n  2. Positional Encoding")
print(f"     - Adds position information to embeddings")
print(f"     - Max length: {max(MODEL_CONFIG['max_input_len'], MODEL_CONFIG['max_target_len']) + 10}")

print(f"\n  3. Encoder (x{MODEL_CONFIG['n_encoder_layers']} layers)")
print(f"     Each layer contains:")
print(f"     - Multi-Head Self-Attention ({MODEL_CONFIG['n_heads']} heads)")
print(f"     - Feed-Forward Network (d_ff={MODEL_CONFIG['d_ff']})")
print(f"     - Layer Normalization (x2)")
print(f"     - Residual Connections (x2)")

print(f"\n  4. Decoder (x{MODEL_CONFIG['n_decoder_layers']} layers)")
print(f"     Each layer contains:")
print(f"     - Masked Multi-Head Self-Attention ({MODEL_CONFIG['n_heads']} heads)")
print(f"     - Multi-Head Cross-Attention ({MODEL_CONFIG['n_heads']} heads)")
print(f"     - Feed-Forward Network (d_ff={MODEL_CONFIG['d_ff']})")
print(f"     - Layer Normalization (x3)")
print(f"     - Residual Connections (x3)")

print(f"\n  5. Output Projection")
print(f"     - Projects decoder output to vocabulary size")
print(f"     - Output: Logits over vocabulary")

print("\n" + "=" * 80)
print("‚úÖ TASK 2 COMPLETE - MODEL ARCHITECTURE")
print("=" * 80)
print("‚úì Transformer encoder-decoder built from scratch")
print("‚úì Multi-head attention implemented")
print("‚úì Positional encoding implemented")
print("‚úì Feed-forward networks implemented")
print("‚úì Layer normalization and residual connections implemented")
print("‚úì All model weights randomly initialized (no pretrained weights)")
print("\nModel is ready for Task 3: Training")
print("=" * 80)

# ============================================================================
# CELL 3: TASK 3 - TRAINING & HYPERPARAMETERS
# ============================================================================
# Empathetic Conversational Chatbot - Transformer Training
#
# This cell handles model training including:
# - Creating DataLoaders with padding and batching
# - Setting up Adam optimizer with specified betas
# - Implementing teacher forcing during training
# - Computing loss with label smoothing
# - Training loop with validation
# - Tracking metrics: BLEU, ROUGE-L, chrF, Perplexity
# - Saving best model based on validation BLEU score
# ============================================================================

# ----------------------------------------------------------------------------
# STEP 1: Install Required Libraries for Evaluation Metrics
# ----------------------------------------------------------------------------
print("=" * 80)
print("INSTALLING EVALUATION LIBRARIES...")
print("=" * 80)

!pip install sacrebleu rouge-score -q

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import numpy as np
import pickle
import json
import time
from tqdm import tqdm
import matplotlib.pyplot as plt
import sacrebleu
from rouge_score import rouge_scorer
import math

print("‚úì All libraries imported successfully!")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 2: Load Preprocessed Data and Model
# ----------------------------------------------------------------------------
print("=" * 80)
print("LOADING PREPROCESSED DATA AND MODEL")
print("=" * 80)

# Load preprocessed data
with open('/content/preprocessed_data/train_processed.pkl', 'rb') as f:
    train_data = pickle.load(f)

with open('/content/preprocessed_data/val_processed.pkl', 'rb') as f:
    val_data = pickle.load(f)

with open('/content/preprocessed_data/config.json', 'r') as f:
    config = json.load(f)

with open('/content/preprocessed_data/vocab.json', 'r') as f:
    vocab = json.load(f)

with open('/content/preprocessed_data/id_to_token.json', 'r') as f:
    id_to_token = json.load(f)
    # Convert string keys back to integers
    id_to_token = {int(k): v for k, v in id_to_token.items()}

print(f"‚úì Training samples: {len(train_data)}")
print(f"‚úì Validation samples: {len(val_data)}")
print(f"‚úì Vocabulary size: {config['vocab_size']}")
print(f"‚úì Max input length: {config['suggested_max_input_len']}")
print(f"‚úì Max target length: {config['suggested_max_target_len']}")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 3: Create Dataset Class
# ----------------------------------------------------------------------------
print("=" * 80)
print("CREATING DATASET AND DATALOADER")
print("=" * 80)

class EmpatheticDialoguesDataset(Dataset):
    """
    Custom Dataset for Empathetic Dialogues.
    Handles tokenized input-output pairs.
    """

    def __init__(self, data, max_input_len, max_target_len):
        """
        Args:
            data: List of dictionaries with 'input_ids' and 'target_ids'
            max_input_len: Maximum input sequence length
            max_target_len: Maximum target sequence length
        """
        self.data = data
        self.max_input_len = max_input_len
        self.max_target_len = max_target_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        """
        Returns:
            input_ids: Tensor of input token IDs
            target_ids: Tensor of target token IDs
        """
        item = self.data[idx]

        # Truncate sequences if they exceed max length
        input_ids = item['input_ids'][:self.max_input_len]
        target_ids = item['target_ids'][:self.max_target_len]

        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'target_ids': torch.tensor(target_ids, dtype=torch.long)
        }


def collate_fn(batch, pad_token_id=0):
    """
    Custom collate function to pad sequences in a batch to the same length.

    Args:
        batch: List of samples from dataset
        pad_token_id: ID of padding token

    Returns:
        Dictionary with padded input and target tensors
    """
    # Extract input and target sequences
    input_ids = [item['input_ids'] for item in batch]
    target_ids = [item['target_ids'] for item in batch]

    # Pad sequences to the longest in the batch
    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)
    target_ids_padded = pad_sequence(target_ids, batch_first=True, padding_value=pad_token_id)

    return {
        'input_ids': input_ids_padded,
        'target_ids': target_ids_padded
    }


# Create datasets
max_input_len = config['suggested_max_input_len']
max_target_len = config['suggested_max_target_len']
pad_token_id = config['special_tokens']['pad_id']

train_dataset = EmpatheticDialoguesDataset(train_data, max_input_len, max_target_len)
val_dataset = EmpatheticDialoguesDataset(val_data, max_input_len, max_target_len)

print(f"‚úì Training dataset size: {len(train_dataset)}")
print(f"‚úì Validation dataset size: {len(val_dataset)}")


# ----------------------------------------------------------------------------
# STEP 4: Training Hyperparameters
# ----------------------------------------------------------------------------
print("\n" + "=" * 80)
print("TRAINING HYPERPARAMETERS")
print("=" * 80)

# Hyperparameters as specified in the task
BATCH_SIZE = 64  # Can be 32 or 64
LEARNING_RATE = 2e-4  # 1e-4 to 5e-4 range
ADAM_BETAS = (0.9, 0.98)  # As specified
NUM_EPOCHS = 20  # Can be adjusted based on convergence
WARMUP_STEPS = 4000  # Learning rate warmup
LABEL_SMOOTHING = 0.1  # Label smoothing for better generalization
GRADIENT_CLIP = 1.0  # Gradient clipping to prevent exploding gradients
PRINT_EVERY = 100  # Print training stats every N batches
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"Batch size: {BATCH_SIZE}")
print(f"Learning rate: {LEARNING_RATE}")
print(f"Adam betas: {ADAM_BETAS}")
print(f"Number of epochs: {NUM_EPOCHS}")
print(f"Warmup steps: {WARMUP_STEPS}")
print(f"Label smoothing: {LABEL_SMOOTHING}")
print(f"Gradient clipping: {GRADIENT_CLIP}")
print(f"Device: {DEVICE}")
print("=" * 80 + "\n")


# Create DataLoaders
train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=lambda x: collate_fn(x, pad_token_id),
    num_workers=2,
    pin_memory=True if torch.cuda.is_available() else False
)

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=lambda x: collate_fn(x, pad_token_id),
    num_workers=2,
    pin_memory=True if torch.cuda.is_available() else False
)

print(f"‚úì Training batches: {len(train_loader)}")
print(f"‚úì Validation batches: {len(val_loader)}")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 5: Initialize Model and Optimizer
# ----------------------------------------------------------------------------
print("=" * 80)
print("INITIALIZING MODEL AND OPTIMIZER")
print("=" * 80)

# Note: The model should already be defined from Task 2
# If not in the same session, you'll need to redefine it or load it
# Here we assume the TransformerModel class and model instance exist from Task 2

# Move model to device
model = model.to(DEVICE)

# Initialize optimizer with specified parameters
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=LEARNING_RATE,
    betas=ADAM_BETAS,
    eps=1e-9
)

# Loss function with label smoothing
criterion = nn.CrossEntropyLoss(
    ignore_index=pad_token_id,
    label_smoothing=LABEL_SMOOTHING
)

print(f"‚úì Model moved to {DEVICE}")
print(f"‚úì Optimizer: Adam with lr={LEARNING_RATE}, betas={ADAM_BETAS}")
print(f"‚úì Loss: CrossEntropyLoss with label_smoothing={LABEL_SMOOTHING}")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 6: Learning Rate Scheduler (Warmup + Decay)
# ----------------------------------------------------------------------------

class WarmupScheduler:
    """
    Learning rate scheduler with warmup and inverse square root decay.
    This is the schedule used in the original Transformer paper.
    """

    def __init__(self, optimizer, d_model, warmup_steps):
        """
        Args:
            optimizer: PyTorch optimizer
            d_model: Model dimension
            warmup_steps: Number of warmup steps
        """
        self.optimizer = optimizer
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.current_step = 0

    def step(self):
        """Update learning rate"""
        self.current_step += 1
        lr = self.get_lr()
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

    def get_lr(self):
        """Calculate learning rate based on current step"""
        step = self.current_step
        # Warmup: linear increase
        # After warmup: inverse square root decay
        lr = (self.d_model ** -0.5) * min(step ** -0.5, step * (self.warmup_steps ** -1.5))
        return lr


# Initialize scheduler
scheduler = WarmupScheduler(optimizer, d_model=512, warmup_steps=WARMUP_STEPS)

print("‚úì Learning rate scheduler initialized (warmup + decay)")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 7: Evaluation Metrics Functions
# ----------------------------------------------------------------------------
print("=" * 80)
print("DEFINING EVALUATION METRICS")
print("=" * 80)

def ids_to_text(ids, id_to_token, special_tokens={'<pad>', '<bos>', '<eos>', '<unk>', '<sep>'}):
    """
    Convert token IDs to text, removing special tokens.

    Args:
        ids: List or tensor of token IDs
        id_to_token: Dictionary mapping ID to token
        special_tokens: Set of special tokens to remove

    Returns:
        Text string
    """
    if isinstance(ids, torch.Tensor):
        ids = ids.cpu().numpy()

    tokens = []
    for id in ids:
        token = id_to_token.get(int(id), '<unk>')
        # Stop at <eos> token
        if token == '<eos>':
            break
        # Skip special tokens
        if token not in special_tokens:
            tokens.append(token)

    return ' '.join(tokens)


def calculate_bleu(predictions, references):
    """
    Calculate BLEU score using sacrebleu.

    Args:
        predictions: List of predicted sentences
        references: List of reference sentences

    Returns:
        BLEU score (0-100)
    """
    # sacrebleu expects references as list of lists
    references = [[ref] for ref in references]
    bleu = sacrebleu.corpus_bleu(predictions, references)
    return bleu.score


def calculate_rouge(predictions, references):
    """
    Calculate ROUGE-L score.

    Args:
        predictions: List of predicted sentences
        references: List of reference sentences

    Returns:
        ROUGE-L F1 score (0-1)
    """
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    scores = []

    for pred, ref in zip(predictions, references):
        score = scorer.score(ref, pred)
        scores.append(score['rougeL'].fmeasure)

    return np.mean(scores)


def calculate_chrf(predictions, references):
    """
    Calculate chrF score using sacrebleu.

    Args:
        predictions: List of predicted sentences
        references: List of reference sentences

    Returns:
        chrF score (0-100)
    """
    references = [[ref] for ref in references]
    chrf = sacrebleu.corpus_chrf(predictions, references)
    return chrf.score


def calculate_perplexity(loss):
    """
    Calculate perplexity from cross-entropy loss.

    Args:
        loss: Cross-entropy loss value

    Returns:
        Perplexity
    """
    return math.exp(loss)


print("‚úì BLEU metric defined")
print("‚úì ROUGE-L metric defined")
print("‚úì chrF metric defined")
print("‚úì Perplexity metric defined")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 8: Training Function
# ----------------------------------------------------------------------------
print("=" * 80)
print("DEFINING TRAINING FUNCTION")
print("=" * 80)

def train_epoch(model, train_loader, optimizer, criterion, scheduler, device, epoch, print_every=100):
    """
    Train the model for one epoch.
    Uses teacher forcing: feeding ground truth tokens to decoder during training.

    Args:
        model: Transformer model
        train_loader: Training data loader
        optimizer: Optimizer
        criterion: Loss function
        scheduler: Learning rate scheduler
        device: Device to train on
        epoch: Current epoch number
        print_every: Print stats every N batches

    Returns:
        Average loss for the epoch
    """
    model.train()
    total_loss = 0
    batch_count = 0

    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch}")

    for batch_idx, batch in enumerate(progress_bar):
        # Move batch to device
        src = batch['input_ids'].to(device)  # (batch_size, src_len)
        tgt = batch['target_ids'].to(device)  # (batch_size, tgt_len)

        # Teacher forcing: use ground truth as decoder input
        # Decoder input: all tokens except the last one (shift right)
        tgt_input = tgt[:, :-1]  # Remove last token

        # Decoder target: all tokens except the first one (ground truth for prediction)
        tgt_output = tgt[:, 1:]  # Remove <bos> token

        # Forward pass
        optimizer.zero_grad()

        # Model outputs logits: (batch_size, tgt_len-1, vocab_size)
        output = model(src, tgt_input)

        # Reshape for loss calculation
        # output: (batch_size * tgt_len, vocab_size)
        # tgt_output: (batch_size * tgt_len)
        output = output.reshape(-1, output.size(-1))
        tgt_output = tgt_output.reshape(-1)

        # Calculate loss
        loss = criterion(output, tgt_output)

        # Backward pass
        loss.backward()

        # Gradient clipping to prevent exploding gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)

        # Update weights
        optimizer.step()

        # Update learning rate
        scheduler.step()

        # Track loss
        total_loss += loss.item()
        batch_count += 1

        # Update progress bar
        current_lr = optimizer.param_groups[0]['lr']
        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'avg_loss': f'{total_loss/batch_count:.4f}',
            'lr': f'{current_lr:.6f}'
        })

        # Print detailed stats periodically
        if (batch_idx + 1) % print_every == 0:
            avg_loss = total_loss / batch_count
            perplexity = calculate_perplexity(avg_loss)
            print(f"\n  Batch {batch_idx+1}/{len(train_loader)} | "
                  f"Loss: {loss.item():.4f} | "
                  f"Avg Loss: {avg_loss:.4f} | "
                  f"Perplexity: {perplexity:.2f} | "
                  f"LR: {current_lr:.6f}")

    avg_loss = total_loss / batch_count
    return avg_loss


print("‚úì Training function defined (with teacher forcing)")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 9: Validation Function
# ----------------------------------------------------------------------------
print("=" * 80)
print("DEFINING VALIDATION FUNCTION")
print("=" * 80)

def validate(model, val_loader, criterion, device, id_to_token, max_samples=500):
    """
    Validate the model and calculate metrics.

    Args:
        model: Transformer model
        val_loader: Validation data loader
        criterion: Loss function
        device: Device
        id_to_token: Dictionary to convert IDs to tokens
        max_samples: Maximum samples to use for BLEU/ROUGE (for speed)

    Returns:
        Dictionary with validation metrics
    """
    model.eval()
    total_loss = 0
    batch_count = 0

    predictions = []
    references = []

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(val_loader, desc="Validating")):
            src = batch['input_ids'].to(device)
            tgt = batch['target_ids'].to(device)

            # Same as training: teacher forcing for loss calculation
            tgt_input = tgt[:, :-1]
            tgt_output = tgt[:, 1:]

            # Forward pass
            output = model(src, tgt_input)

            # Calculate loss
            output_flat = output.reshape(-1, output.size(-1))
            tgt_flat = tgt_output.reshape(-1)
            loss = criterion(output_flat, tgt_flat)

            total_loss += loss.item()
            batch_count += 1

            # Generate predictions for BLEU/ROUGE (on subset of data for speed)
            if len(predictions) < max_samples:
                # Get predicted tokens (greedy decoding)
                pred_ids = output.argmax(dim=-1)  # (batch_size, tgt_len-1)

                # Convert to text
                for i in range(min(src.size(0), max_samples - len(predictions))):
                    pred_text = ids_to_text(pred_ids[i], id_to_token)
                    ref_text = ids_to_text(tgt[i], id_to_token)

                    predictions.append(pred_text)
                    references.append(ref_text)

    # Calculate metrics
    avg_loss = total_loss / batch_count
    perplexity = calculate_perplexity(avg_loss)
    bleu_score = calculate_bleu(predictions, references)
    rouge_score = calculate_rouge(predictions, references)
    chrf_score = calculate_chrf(predictions, references)

    metrics = {
        'loss': avg_loss,
        'perplexity': perplexity,
        'bleu': bleu_score,
        'rouge_l': rouge_score,
        'chrf': chrf_score
    }

    return metrics


print("‚úì Validation function defined")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 10: Training Loop
# ----------------------------------------------------------------------------
print("=" * 80)
print("STARTING TRAINING")
print("=" * 80)
print(f"Training for {NUM_EPOCHS} epochs...")
print(f"Saving best model based on validation BLEU score")
print("=" * 80 + "\n")

# Track training history
history = {
    'train_loss': [],
    'val_loss': [],
    'val_perplexity': [],
    'val_bleu': [],
    'val_rouge_l': [],
    'val_chrf': []
}

# Best model tracking
best_bleu = 0.0
best_epoch = 0
patience = 5  # Early stopping patience
patience_counter = 0

# Training loop
for epoch in range(1, NUM_EPOCHS + 1):
    print(f"\n{'='*80}")
    print(f"EPOCH {epoch}/{NUM_EPOCHS}")
    print(f"{'='*80}")

    # Train
    start_time = time.time()
    train_loss = train_epoch(model, train_loader, optimizer, criterion, scheduler, DEVICE, epoch, PRINT_EVERY)
    train_time = time.time() - start_time

    # Validate
    start_time = time.time()
    val_metrics = validate(model, val_loader, criterion, DEVICE, id_to_token)
    val_time = time.time() - start_time

    # Update history
    history['train_loss'].append(train_loss)
    history['val_loss'].append(val_metrics['loss'])
    history['val_perplexity'].append(val_metrics['perplexity'])
    history['val_bleu'].append(val_metrics['bleu'])
    history['val_rouge_l'].append(val_metrics['rouge_l'])
    history['val_chrf'].append(val_metrics['chrf'])

    # Print epoch summary
    print(f"\n{'='*80}")
    print(f"EPOCH {epoch} SUMMARY")
    print(f"{'='*80}")
    print(f"Training Loss: {train_loss:.4f} | Time: {train_time:.2f}s")
    print(f"Validation Loss: {val_metrics['loss']:.4f} | Time: {val_time:.2f}s")
    print(f"Validation Perplexity: {val_metrics['perplexity']:.2f}")
    print(f"Validation BLEU: {val_metrics['bleu']:.2f}")
    print(f"Validation ROUGE-L: {val_metrics['rouge_l']:.4f}")
    print(f"Validation chrF: {val_metrics['chrf']:.2f}")
    print(f"{'='*80}")

    # Save best model based on BLEU score
    if val_metrics['bleu'] > best_bleu:
        best_bleu = val_metrics['bleu']
        best_epoch = epoch
        patience_counter = 0

        # Save model
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'train_loss': train_loss,
            'val_metrics': val_metrics,
            'history': history,
            'config': config
        }, '/content/best_model.pt')

        print(f"\nüéâ New best model saved! BLEU: {best_bleu:.2f}")
    else:
        patience_counter += 1
        print(f"\nNo improvement. Patience: {patience_counter}/{patience}")

    # Early stopping
    if patience_counter >= patience:
        print(f"\n‚ö†Ô∏è Early stopping triggered after {epoch} epochs")
        print(f"Best BLEU score: {best_bleu:.2f} at epoch {best_epoch}")
        break

print("\n" + "=" * 80)
print("TRAINING COMPLETE!")
print("=" * 80)
print(f"Best BLEU score: {best_bleu:.2f} achieved at epoch {best_epoch}")
print(f"Best model saved at: /content/best_model.pt")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 11: Plot Training History
# ----------------------------------------------------------------------------
print("=" * 80)
print("PLOTTING TRAINING HISTORY")
print("=" * 80)

fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# Plot 1: Training and Validation Loss
axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')
axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s')
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].set_title('Training and Validation Loss')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Plot 2: Validation Perplexity
axes[0, 1].plot(history['val_perplexity'], label='Val Perplexity', marker='o', color='orange')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('Perplexity')
axes[0, 1].set_title('Validation Perplexity')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Plot 3: Validation BLEU
axes[0, 2].plot(history['val_bleu'], label='Val BLEU', marker='o', color='green')
axes[0, 2].axhline(y=best_bleu, color='red', linestyle='--', label=f'Best: {best_bleu:.2f}')
axes[0, 2].set_xlabel('Epoch')
axes[0, 2].set_ylabel('BLEU Score')
axes[0, 2].set_title('Validation BLEU Score')
axes[0, 2].legend()
axes[0, 2].grid(True, alpha=0.3)

# Plot 4: Validation ROUGE-L
axes[1, 0].plot(history['val_rouge_l'], label='Val ROUGE-L', marker='o', color='purple')
axes[1, 0].set_xlabel('Epoch')
axes[1, 0].set_ylabel('ROUGE-L Score')
axes[1, 0].set_title('Validation ROUGE-L Score')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# Plot 5: Validation chrF
axes[1, 1].plot(history['val_chrf'], label='Val chrF', marker='o', color='brown')
axes[1, 1].set_xlabel('Epoch')
axes[1, 1].set_ylabel('chrF Score')
axes[1, 1].set_title('Validation chrF Score')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

# Plot 6: All metrics combined (normalized)
axes[1, 2].plot(np.array(history['val_bleu']) / 100, label='BLEU (norm)', marker='o')
axes[1, 2].plot(history['val_rouge_l'], label='ROUGE-L', marker='s')
axes[1, 2].plot(np.array(history['val_chrf']) / 100, label='chrF (norm)', marker='^')
axes[1, 2].set_xlabel('Epoch')
axes[1, 2].set_ylabel('Score (normalized)')
axes[1, 2].set_title('All Metrics (Normalized)')
axes[1, 2].legend()
axes[1, 2].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('/content/training_history.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Training history plots saved to: /content/training_history.png")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 12: Final Summary
# ----------------------------------------------------------------------------
print("=" * 80)
print("‚úÖ TASK 3 COMPLETE - TRAINING SUMMARY")
print("=" * 80)

print("\nüìä TRAINING CONFIGURATION:")
print(f"   ‚Ä¢ Batch size: {BATCH_SIZE}")
print(f"   ‚Ä¢ Optimizer: Adam (betas={ADAM_BETAS})")
print(f"   ‚Ä¢ Learning rate: {LEARNING_RATE} (with warmup)")
print(f"   ‚Ä¢ Epochs trained: {len(history['train_loss'])}")
print(f"   ‚Ä¢ Teacher forcing: ‚úì Enabled")
print(f"   ‚Ä¢ Label smoothing: {LABEL_SMOOTHING}")
print(f"   ‚Ä¢ Gradient clipping: {GRADIENT_CLIP}")

print("\nüìà BEST MODEL PERFORMANCE (Epoch {}):" .format(best_epoch))
print(f"   ‚Ä¢ BLEU Score: {best_bleu:.2f}")
print(f"   ‚Ä¢ ROUGE-L Score: {max(history['val_rouge_l']):.4f}")
print(f"   ‚Ä¢ chrF Score: {max(history['val_chrf']):.2f}")
print(f"   ‚Ä¢ Perplexity: {min(history['val_perplexity']):.2f}")

print("\nüìÅ SAVED FILES:")
print("   ‚Ä¢ Best model: /content/best_model.pt")
print("   ‚Ä¢ Training plots: /content/training_history.png")

print("\n‚úÖ REQUIREMENTS MET:")
print("   ‚úì Batch size: 32/64 (using {})".format(BATCH_SIZE))
print("   ‚úì Optimizer: Adam with betas=(0.9, 0.98)")
print("   ‚úì Learning rate: 1e-4 to 5e-4 range")
print("   ‚úì Teacher forcing: Implemented")
print("   ‚úì Best model saved based on validation BLEU")
print("   ‚úì Metrics tracked: BLEU, ROUGE-L, chrF, Perplexity")

print("\n" + "=" * 80)
print("üéâ TRAINING COMPLETE!")
print("=" * 80)
print("Model is ready for Task 4: Evaluation")
print("=" * 80)

# ============================================================================
# CELL 4: TASK 4 - EVALUATION
# ============================================================================
# Empathetic Conversational Chatbot - Model Evaluation
#
# This cell handles comprehensive model evaluation including:
# - Loading best trained model
# - Evaluation on test set with all metrics
# - Automatic Metrics: BLEU, ROUGE-L, chrF, Perplexity
# - Qualitative examples comparing model output vs ground truth
# - Human evaluation preparation: Fluency, Relevance, Adequacy
# - Attention visualization
# - Detailed analysis and insights
# ============================================================================

# ----------------------------------------------------------------------------
# STEP 1: Import Required Libraries
# ----------------------------------------------------------------------------
print("=" * 80)
print("IMPORTING LIBRARIES FOR EVALUATION")
print("=" * 80)

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import pickle
import json
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import sacrebleu
from rouge_score import rouge_scorer
import math
import random

print("‚úì All libraries imported successfully!")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 2: Load Preprocessed Data and Configuration
# ----------------------------------------------------------------------------
print("=" * 80)
print("LOADING PREPROCESSED DATA")
print("=" * 80)

# Load test data
with open('/content/preprocessed_data/test_processed.pkl', 'rb') as f:
    test_data = pickle.load(f)

# Load configuration
with open('/content/preprocessed_data/config.json', 'r') as f:
    config = json.load(f)

# Load vocabulary
with open('/content/preprocessed_data/vocab.json', 'r') as f:
    vocab = json.load(f)

with open('/content/preprocessed_data/id_to_token.json', 'r') as f:
    id_to_token = json.load(f)
    # Convert string keys back to integers
    id_to_token = {int(k): v for k, v in id_to_token.items()}

# Load original test dataframe for reference
test_df = pd.read_csv('/content/preprocessed_data/test_df.csv')

print(f"‚úì Test samples: {len(test_data)}")
print(f"‚úì Vocabulary size: {config['vocab_size']}")
print(f"‚úì Special tokens loaded")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 3: Load Best Trained Model
# ----------------------------------------------------------------------------
print("=" * 80)
print("LOADING BEST TRAINED MODEL")
print("=" * 80)

# Load checkpoint
# Note: weights_only=False is safe here since this is our own trained model
checkpoint = torch.load('/content/best_model.pt', map_location='cpu', weights_only=False)

print(f"Best model from epoch: {checkpoint['epoch']}")
print(f"Training loss: {checkpoint['train_loss']:.4f}")
print(f"Validation BLEU: {checkpoint['val_metrics']['bleu']:.2f}")
print(f"Validation perplexity: {checkpoint['val_metrics']['perplexity']:.2f}")

# Note: Model should already be defined from Task 2
# If not, you'll need to redefine the TransformerModel class
# Here we assume 'model' variable exists from previous cells

# Load model weights
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.load_state_dict(checkpoint['model_state_dict'])
model = model.to(device)
model.eval()

print(f"\n‚úì Model loaded and set to evaluation mode")
print(f"‚úì Device: {device}")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 4: Define Evaluation Helper Functions
# ----------------------------------------------------------------------------
print("=" * 80)
print("DEFINING EVALUATION FUNCTIONS")
print("=" * 80)

def ids_to_text(ids, id_to_token, special_tokens={'<pad>', '<bos>', '<eos>', '<unk>', '<sep>'}):
    """
    Convert token IDs to text, removing special tokens.

    Args:
        ids: List or tensor of token IDs
        id_to_token: Dictionary mapping ID to token
        special_tokens: Set of special tokens to remove

    Returns:
        Text string
    """
    if isinstance(ids, torch.Tensor):
        ids = ids.cpu().numpy()

    tokens = []
    for id in ids:
        token = id_to_token.get(int(id), '<unk>')
        # Stop at <eos> token
        if token == '<eos>':
            break
        # Skip special tokens
        if token not in special_tokens:
            tokens.append(token)

    return ' '.join(tokens)


def greedy_decode(model, src, max_len, bos_id, eos_id, pad_id, device):
    """
    Greedy decoding: always select the most probable token at each step.

    Args:
        model: Transformer model
        src: Source sequence (batch_size, src_len)
        max_len: Maximum length to generate
        bos_id: Beginning of sequence token ID
        eos_id: End of sequence token ID
        pad_id: Padding token ID
        device: Device

    Returns:
        Generated sequence (batch_size, max_len)
    """
    model.eval()
    batch_size = src.size(0)

    # Encode source
    src_mask = model.create_padding_mask(src)
    encoder_output = model.encode(src, src_mask)

    # Initialize decoder input with <bos>
    tgt = torch.full((batch_size, 1), bos_id, dtype=torch.long).to(device)

    # Generate tokens one by one
    for _ in range(max_len - 1):
        # Create target mask
        tgt_padding_mask = model.create_padding_mask(tgt)
        tgt_look_ahead_mask = model.create_look_ahead_mask(tgt.size(1)).to(device)
        tgt_mask = tgt_padding_mask * tgt_look_ahead_mask

        # Decode
        decoder_output = model.decode(tgt, encoder_output, src_mask, tgt_mask)

        # Project to vocabulary
        output = model.output_projection(decoder_output)

        # Get last token prediction
        next_token_logits = output[:, -1, :]  # (batch_size, vocab_size)
        next_token = next_token_logits.argmax(dim=-1, keepdim=True)  # (batch_size, 1)

        # Append to target
        tgt = torch.cat([tgt, next_token], dim=1)

        # Stop if all sequences have generated <eos>
        if (next_token == eos_id).all():
            break

    return tgt


def calculate_bleu(predictions, references):
    """Calculate BLEU score using sacrebleu."""
    references = [[ref] for ref in references]
    bleu = sacrebleu.corpus_bleu(predictions, references)
    return bleu.score


def calculate_rouge(predictions, references):
    """Calculate ROUGE-L score."""
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    scores = []

    for pred, ref in zip(predictions, references):
        score = scorer.score(ref, pred)
        scores.append(score['rougeL'].fmeasure)

    return np.mean(scores)


def calculate_chrf(predictions, references):
    """Calculate chrF score using sacrebleu."""
    references = [[ref] for ref in references]
    chrf = sacrebleu.corpus_chrf(predictions, references)
    return chrf.score


def calculate_perplexity(loss):
    """Calculate perplexity from cross-entropy loss."""
    return math.exp(loss)


print("‚úì Greedy decoding function defined")
print("‚úì Text conversion function defined")
print("‚úì All metric functions defined")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 5: Evaluate on Test Set
# ----------------------------------------------------------------------------
print("=" * 80)
print("EVALUATING ON TEST SET")
print("=" * 80)

# Get special token IDs
pad_id = config['special_tokens']['pad_id']
bos_id = config['special_tokens']['bos_id']
eos_id = config['special_tokens']['eos_id']
max_target_len = config['suggested_max_target_len']
max_input_len = config['suggested_max_input_len']

# Loss function
criterion = nn.CrossEntropyLoss(ignore_index=pad_id)

# Storage for results
predictions = []
references = []
total_loss = 0
batch_count = 0

# For detailed examples
detailed_examples = []

print(f"Evaluating {len(test_data)} test samples...")
print("Generating predictions using greedy decoding...\n")

# Create batches manually for test set
batch_size = 32

with torch.no_grad():
    for i in tqdm(range(0, len(test_data), batch_size), desc="Evaluating"):
        # Get batch
        batch = test_data[i:i+batch_size]

        # Prepare input and target
        src_batch = []
        tgt_batch = []

        for sample in batch:
            # Truncate sequences to max lengths used during training
            input_ids = sample['input_ids'][:max_input_len]
            target_ids = sample['target_ids'][:max_target_len]

            src_batch.append(torch.tensor(input_ids, dtype=torch.long))
            tgt_batch.append(torch.tensor(target_ids, dtype=torch.long))

        # Pad sequences
        from torch.nn.utils.rnn import pad_sequence
        src = pad_sequence(src_batch, batch_first=True, padding_value=pad_id).to(device)
        tgt = pad_sequence(tgt_batch, batch_first=True, padding_value=pad_id).to(device)

        # Calculate loss (for perplexity)
        tgt_input = tgt[:, :-1]
        tgt_output = tgt[:, 1:]

        output = model(src, tgt_input)
        output_flat = output.reshape(-1, output.size(-1))
        tgt_flat = tgt_output.reshape(-1)
        loss = criterion(output_flat, tgt_flat)

        total_loss += loss.item()
        batch_count += 1

        # Generate predictions using greedy decoding
        generated = greedy_decode(
            model, src, max_target_len, bos_id, eos_id, pad_id, device
        )

        # Convert to text
        for j in range(len(batch)):
            pred_text = ids_to_text(generated[j], id_to_token)
            ref_text = ids_to_text(tgt[j], id_to_token)

            predictions.append(pred_text)
            references.append(ref_text)

            # Store first 20 examples for detailed analysis
            if len(detailed_examples) < 20:
                idx = i + j
                detailed_examples.append({
                    'emotion': test_df.iloc[idx]['emotion_normalized'],
                    'situation': test_df.iloc[idx]['situation_normalized'],
                    'customer': test_df.iloc[idx]['customer_normalized'],
                    'reference': ref_text,
                    'prediction': pred_text
                })

print("\n‚úì Evaluation complete!")


# ----------------------------------------------------------------------------
# STEP 6: Calculate All Metrics
# ----------------------------------------------------------------------------
print("\n" + "=" * 80)
print("CALCULATING AUTOMATIC METRICS")
print("=" * 80)

# Calculate metrics
avg_loss = total_loss / batch_count
perplexity = calculate_perplexity(avg_loss)
bleu_score = calculate_bleu(predictions, references)
rouge_score = calculate_rouge(predictions, references)
chrf_score = calculate_chrf(predictions, references)

print(f"\nüìä TEST SET RESULTS:")
print(f"{'='*80}")
print(f"Test Loss: {avg_loss:.4f}")
print(f"Perplexity: {perplexity:.2f}")
print(f"BLEU Score: {bleu_score:.2f}")
print(f"ROUGE-L Score: {rouge_score:.4f}")
print(f"chrF Score: {chrf_score:.2f}")
print(f"{'='*80}")


# ----------------------------------------------------------------------------
# STEP 7: Display Qualitative Examples
# ----------------------------------------------------------------------------
print("\n" + "=" * 80)
print("QUALITATIVE EXAMPLES - MODEL vs GROUND TRUTH")
print("=" * 80)

# Show diverse examples
for i, example in enumerate(detailed_examples[:10], 1):
    print(f"\n{'‚îÄ'*80}")
    print(f"EXAMPLE {i}")
    print(f"{'‚îÄ'*80}")
    print(f"Emotion: {example['emotion']}")
    print(f"Situation: {example['situation'][:80]}{'...' if len(example['situation']) > 80 else ''}")
    print(f"Customer: {example['customer']}")
    print(f"\n‚Üí Ground Truth: {example['reference']}")
    print(f"‚Üí Model Output: {example['prediction']}")

    # Simple quality assessment
    ref_words = set(example['reference'].split())
    pred_words = set(example['prediction'].split())
    overlap = len(ref_words & pred_words) / max(len(ref_words), 1)
    print(f"‚Üí Word Overlap: {overlap:.2%}")

print("\n" + "=" * 80)


# ----------------------------------------------------------------------------
# STEP 8: Human Evaluation Preparation
# ----------------------------------------------------------------------------
print("\n" + "=" * 80)
print("HUMAN EVALUATION - Sample Questionnaire")
print("=" * 80)

print("""
Human evaluators will rate model outputs on a 1-5 scale for:

1. FLUENCY: Is the response grammatically correct and natural?
   1 = Incomprehensible, 2 = Poor, 3 = Acceptable, 4 = Good, 5 = Perfect

2. RELEVANCE: Is the response relevant to the context (emotion, situation, customer)?
   1 = Completely irrelevant, 2 = Slightly relevant, 3 = Moderately relevant,
   4 = Relevant, 5 = Highly relevant

3. ADEQUACY: Does the response appropriately address the customer's utterance?
   1 = Completely inadequate, 2 = Poor, 3 = Acceptable, 4 = Good, 5 = Excellent

4. EMPATHY: Does the response show empathy appropriate to the emotion?
   1 = No empathy, 2 = Slight empathy, 3 = Moderate empathy,
   4 = Good empathy, 5 = Excellent empathy
""")

print("Preparing samples for human evaluation...\n")

# Create human evaluation dataset
human_eval_samples = []
random.seed(42)
eval_indices = random.sample(range(len(detailed_examples)), min(10, len(detailed_examples)))

for idx in eval_indices:
    example = detailed_examples[idx]
    human_eval_samples.append({
        'emotion': example['emotion'],
        'situation': example['situation'][:100],
        'customer_utterance': example['customer'],
        'model_response': example['prediction'],
        'ground_truth': example['reference'],
        'fluency_score': '',
        'relevance_score': '',
        'adequacy_score': '',
        'empathy_score': ''
    })

# Save to CSV for human evaluation
human_eval_df = pd.DataFrame(human_eval_samples)
human_eval_df.to_csv('/content/human_evaluation_samples.csv', index=False)

print("‚úì Human evaluation samples saved to: /content/human_evaluation_samples.csv")
print(f"‚úì Total samples for evaluation: {len(human_eval_samples)}")

# Display sample format
print("\nSample format for human evaluation:")
print("‚îÄ" * 80)
print(human_eval_df.head(3).to_string())
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 9: Metrics Comparison and Visualization
# ----------------------------------------------------------------------------
print("=" * 80)
print("METRICS VISUALIZATION")
print("=" * 80)

# Create visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Plot 1: Metrics Bar Chart
metrics_names = ['BLEU', 'ROUGE-L', 'chrF']
metrics_values = [bleu_score / 100, rouge_score, chrf_score / 100]  # Normalize to 0-1 scale

axes[0, 0].bar(metrics_names, metrics_values, color=['steelblue', 'forestgreen', 'coral'])
axes[0, 0].set_ylabel('Score', fontsize=12)
axes[0, 0].set_title('Test Set Metrics Comparison', fontsize=14, fontweight='bold')
axes[0, 0].set_ylim([0, 1])
axes[0, 0].grid(True, alpha=0.3, axis='y')
for i, v in enumerate(metrics_values):
    axes[0, 0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')

# Plot 2: Perplexity
axes[0, 1].bar(['Perplexity'], [perplexity], color='indianred', width=0.4)
axes[0, 1].set_ylabel('Perplexity', fontsize=12)
axes[0, 1].set_title('Test Set Perplexity', fontsize=14, fontweight='bold')
axes[0, 1].grid(True, alpha=0.3, axis='y')
axes[0, 1].text(0, perplexity + 2, f'{perplexity:.2f}', ha='center', fontweight='bold')

# Plot 3: Response Length Distribution
pred_lengths = [len(p.split()) for p in predictions[:1000]]
ref_lengths = [len(r.split()) for r in references[:1000]]

axes[1, 0].hist(pred_lengths, bins=30, alpha=0.6, label='Predictions', color='steelblue', edgecolor='black')
axes[1, 0].hist(ref_lengths, bins=30, alpha=0.6, label='References', color='orange', edgecolor='black')
axes[1, 0].set_xlabel('Response Length (words)', fontsize=12)
axes[1, 0].set_ylabel('Frequency', fontsize=12)
axes[1, 0].set_title('Response Length Distribution', fontsize=14, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# Plot 4: Summary Statistics Table
summary_stats = [
    ['Metric', 'Score'],
    ['BLEU', f'{bleu_score:.2f}'],
    ['ROUGE-L', f'{rouge_score:.4f}'],
    ['chrF', f'{chrf_score:.2f}'],
    ['Perplexity', f'{perplexity:.2f}'],
    ['Avg Pred Length', f'{np.mean(pred_lengths):.1f} words'],
    ['Avg Ref Length', f'{np.mean(ref_lengths):.1f} words'],
]

axes[1, 1].axis('tight')
axes[1, 1].axis('off')
table = axes[1, 1].table(cellText=summary_stats, cellLoc='left', loc='center',
                          colWidths=[0.5, 0.5])
table.auto_set_font_size(False)
table.set_fontsize(11)
table.scale(1, 2.5)

# Style header row
for i in range(2):
    table[(0, i)].set_facecolor('#4CAF50')
    table[(0, i)].set_text_props(weight='bold', color='white')

# Style data rows
for i in range(1, len(summary_stats)):
    for j in range(2):
        table[(i, j)].set_facecolor('#f0f0f0' if i % 2 == 0 else 'white')

axes[1, 1].set_title('Evaluation Summary', fontsize=14, fontweight='bold', pad=20)

plt.tight_layout()
plt.savefig('/content/evaluation_results.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Evaluation visualizations saved to: /content/evaluation_results.png")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 10: Error Analysis
# ----------------------------------------------------------------------------
print("=" * 80)
print("ERROR ANALYSIS")
print("=" * 80)

# Calculate per-sample BLEU scores to find best and worst examples
from sacrebleu.metrics import BLEU
bleu_metric = BLEU()

sample_scores = []
for pred, ref in zip(predictions[:100], references[:100]):
    score = bleu_metric.sentence_score(pred, [ref]).score
    sample_scores.append(score)

# Find best examples
best_indices = np.argsort(sample_scores)[-5:][::-1]
worst_indices = np.argsort(sample_scores)[:5]

print("\nüéØ BEST PREDICTIONS (Highest BLEU):")
print("‚îÄ" * 80)
for rank, idx in enumerate(best_indices, 1):
    print(f"\n{rank}. BLEU Score: {sample_scores[idx]:.2f}")
    print(f"   Reference: {references[idx]}")
    print(f"   Prediction: {predictions[idx]}")

print("\n\n‚ùå WORST PREDICTIONS (Lowest BLEU):")
print("‚îÄ" * 80)
for rank, idx in enumerate(worst_indices, 1):
    print(f"\n{rank}. BLEU Score: {sample_scores[idx]:.2f}")
    print(f"   Reference: {references[idx]}")
    print(f"   Prediction: {predictions[idx]}")

print("\n" + "=" * 80)


# ----------------------------------------------------------------------------
# STEP 11: Final Summary
# ----------------------------------------------------------------------------
print("\n" + "=" * 80)
print("‚úÖ TASK 4 COMPLETE - EVALUATION SUMMARY")
print("=" * 80)

print("\nüìä AUTOMATIC METRICS (Test Set):")
print(f"   ‚Ä¢ BLEU Score: {bleu_score:.2f}")
print(f"   ‚Ä¢ ROUGE-L Score: {rouge_score:.4f}")
print(f"   ‚Ä¢ chrF Score: {chrf_score:.2f}")
print(f"   ‚Ä¢ Perplexity: {perplexity:.2f}")

print("\nüìà STATISTICAL ANALYSIS:")
print(f"   ‚Ä¢ Test samples evaluated: {len(predictions)}")
print(f"   ‚Ä¢ Average prediction length: {np.mean(pred_lengths):.1f} words")
print(f"   ‚Ä¢ Average reference length: {np.mean(ref_lengths):.1f} words")
print(f"   ‚Ä¢ Shortest prediction: {min(pred_lengths)} words")
print(f"   ‚Ä¢ Longest prediction: {max(pred_lengths)} words")

print("\nüë• HUMAN EVALUATION:")
print(f"   ‚Ä¢ Prepared {len(human_eval_samples)} samples for human evaluation")
print(f"   ‚Ä¢ Evaluation criteria: Fluency, Relevance, Adequacy, Empathy (1-5 scale)")
print(f"   ‚Ä¢ Samples saved to: /content/human_evaluation_samples.csv")

print("\nüìÅ SAVED FILES:")
print("   ‚Ä¢ Evaluation plots: /content/evaluation_results.png")
print("   ‚Ä¢ Human eval samples: /content/human_evaluation_samples.csv")

print("\n‚úÖ REQUIREMENTS MET:")
print("   ‚úì Automatic Metrics: BLEU, ROUGE-L, chrF, Perplexity")
print("   ‚úì Qualitative examples provided (model vs ground truth)")
print("   ‚úì Human evaluation prepared (Fluency, Relevance, Adequacy)")
print("   ‚úì Error analysis conducted")
print("   ‚úì Comprehensive visualizations generated")

print("\n" + "=" * 80)
print("üéâ EVALUATION COMPLETE!")
print("=" * 80)
print("Model is ready for Task 5: Inference & UI")
print("=" * 80)

# ============================================================================
# CELL 5 - PART 1: TASK 5 - INFERENCE & UI (Core Functions)
# ============================================================================
# Empathetic Conversational Chatbot - Inference Functions
#
# This is PART 1 of Task 5, containing:
# - Inference helper functions
# - Greedy decoding
# - Beam search decoding
# - Attention extraction for visualization
#
# PART 2 will contain the Gradio UI interface
# ============================================================================

# ----------------------------------------------------------------------------
# STEP 1: Import Required Libraries
# ----------------------------------------------------------------------------
print("=" * 80)
print("TASK 5 - PART 1: LOADING INFERENCE FUNCTIONS")
print("=" * 80)

import torch
import torch.nn.functional as F
import numpy as np
import json
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Tuple, Dict

print("‚úì Libraries imported successfully!")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 2: Load Model and Configurations
# ----------------------------------------------------------------------------
print("=" * 80)
print("LOADING MODEL FOR INFERENCE")
print("=" * 80)

# Load configurations
with open('/content/preprocessed_data/config.json', 'r') as f:
    config = json.load(f)

with open('/content/preprocessed_data/vocab.json', 'r') as f:
    vocab = json.load(f)

with open('/content/preprocessed_data/id_to_token.json', 'r') as f:
    id_to_token = json.load(f)
    id_to_token = {int(k): v for k, v in id_to_token.items()}

# Get special token IDs
PAD_ID = config['special_tokens']['pad_id']
UNK_ID = config['special_tokens']['unk_id']
BOS_ID = config['special_tokens']['bos_id']
EOS_ID = config['special_tokens']['eos_id']
SEP_ID = config['special_tokens']['sep_id']

# Load best model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Model should already be loaded from previous tasks
# If not, you need to reload it
model.eval()

print(f"‚úì Model loaded in evaluation mode")
print(f"‚úì Device: {device}")
print(f"‚úì Vocabulary size: {len(vocab)}")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 3: Text Processing Functions
# ----------------------------------------------------------------------------
print("=" * 80)
print("DEFINING TEXT PROCESSING FUNCTIONS")
print("=" * 80)

def normalize_text(text):
    """
    Normalize input text (same as during training).

    Args:
        text: Input string

    Returns:
        Normalized string
    """
    if not isinstance(text, str):
        return ""

    # Convert to lowercase
    text = text.lower()

    # Normalize contractions
    contractions = {
        "won't": "will not",
        "can't": "cannot",
        "n't": " not",
        "'re": " are",
        "'ve": " have",
        "'ll": " will",
        "'d": " would",
        "'m": " am"
    }
    for contraction, expansion in contractions.items():
        text = text.replace(contraction, expansion)

    # Add spaces around punctuation
    import re
    text = re.sub(r'([.!?,;:\"\'])', r' \1 ', text)

    # Replace multiple spaces
    text = re.sub(r'\s+', ' ', text)

    return text.strip()


def tokenize(text):
    """Simple word-level tokenization."""
    return text.split()


def tokens_to_ids(tokens, vocab):
    """Convert tokens to IDs."""
    return [vocab.get(token, UNK_ID) for token in tokens]


def ids_to_text(ids, id_to_token, special_tokens={'<pad>', '<bos>', '<eos>', '<unk>', '<sep>'}):
    """Convert IDs back to text."""
    if isinstance(ids, torch.Tensor):
        ids = ids.cpu().numpy()

    tokens = []
    for id in ids:
        token = id_to_token.get(int(id), '<unk>')
        if token == '<eos>':
            break
        if token not in special_tokens:
            tokens.append(token)

    return ' '.join(tokens)


def prepare_input(emotion, situation, customer_utterance, vocab):
    """
    Prepare input sequence in the format expected by the model.

    Format: emotion: {emotion} <sep> situation: {situation} <sep> customer: {customer} <sep>

    Args:
        emotion: Emotion string
        situation: Situation string
        customer_utterance: Customer's message
        vocab: Vocabulary dictionary

    Returns:
        Tensor of input token IDs
    """
    # Normalize inputs
    emotion = normalize_text(emotion) if emotion else "neutral"
    situation = normalize_text(situation) if situation else ""
    customer_utterance = normalize_text(customer_utterance)

    # Create input string
    input_text = f"emotion : {emotion} <sep> situation : {situation} <sep> customer : {customer_utterance} <sep>"

    # Tokenize
    tokens = []
    for token in input_text.split():
        if token == '<sep>':
            tokens.append('<sep>')
        else:
            tokens.append(token)

    # Convert to IDs
    input_ids = tokens_to_ids(tokens, vocab)

    # Truncate to max length
    max_input_len = config['suggested_max_input_len']
    input_ids = input_ids[:max_input_len]

    return torch.tensor(input_ids, dtype=torch.long).unsqueeze(0)  # Add batch dimension


print("‚úì Text processing functions defined")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 4: Greedy Decoding Function
# ----------------------------------------------------------------------------
print("=" * 80)
print("DEFINING GREEDY DECODING")
print("=" * 80)

def greedy_decode(model, src, max_len, bos_id, eos_id, device):
    """
    Greedy decoding: always select the most probable token.

    Args:
        model: Transformer model
        src: Source sequence (1, src_len)
        max_len: Maximum length to generate
        bos_id: Beginning of sequence token ID
        eos_id: End of sequence token ID
        device: Device

    Returns:
        Tuple of (generated sequence, attention weights)
    """
    model.eval()
    src = src.to(device)

    with torch.no_grad():
        # Encode source
        src_mask = model.create_padding_mask(src)
        encoder_output = model.encode(src, src_mask)

        # Initialize decoder input with <bos>
        tgt = torch.full((1, 1), bos_id, dtype=torch.long).to(device)

        # Store attention weights for visualization
        attention_weights = []

        # Generate tokens one by one
        for _ in range(max_len - 1):
            # Create target mask
            tgt_padding_mask = model.create_padding_mask(tgt)
            tgt_look_ahead_mask = model.create_look_ahead_mask(tgt.size(1)).to(device)
            tgt_mask = tgt_padding_mask * tgt_look_ahead_mask

            # Decode
            decoder_output = model.decode(tgt, encoder_output, src_mask, tgt_mask)

            # Project to vocabulary
            output = model.output_projection(decoder_output)

            # Get next token (greedy selection)
            next_token_logits = output[:, -1, :]
            next_token = next_token_logits.argmax(dim=-1, keepdim=True)

            # Append to sequence
            tgt = torch.cat([tgt, next_token], dim=1)

            # Stop if <eos> generated
            if next_token.item() == eos_id:
                break

        return tgt, None  # We'll implement attention extraction later


print("‚úì Greedy decoding function defined")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 5: Beam Search Decoding Function
# ----------------------------------------------------------------------------
print("=" * 80)
print("DEFINING BEAM SEARCH DECODING")
print("=" * 80)

class BeamSearchNode:
    """Node for beam search."""

    def __init__(self, sequence, score, length):
        self.sequence = sequence
        self.score = score
        self.length = length

    def get_avg_score(self):
        """Get average score (normalized by length)."""
        return self.score / self.length


def beam_search_decode(model, src, max_len, bos_id, eos_id, device, beam_width=5):
    """
    Beam search decoding: keep top-k candidates at each step.

    Args:
        model: Transformer model
        src: Source sequence (1, src_len)
        max_len: Maximum length to generate
        bos_id: Beginning of sequence token ID
        eos_id: End of sequence token ID
        device: Device
        beam_width: Number of beams to keep

    Returns:
        Best generated sequence
    """
    model.eval()
    src = src.to(device)

    with torch.no_grad():
        # Encode source
        src_mask = model.create_padding_mask(src)
        encoder_output = model.encode(src, src_mask)

        # Initialize with <bos>
        initial_sequence = torch.full((1, 1), bos_id, dtype=torch.long).to(device)

        # Active beams
        beams = [BeamSearchNode(initial_sequence, 0.0, 1)]
        finished_beams = []

        # Beam search
        for step in range(max_len - 1):
            candidates = []

            for beam in beams:
                tgt = beam.sequence

                # If beam already ended with <eos>, add to finished
                if tgt[0, -1].item() == eos_id:
                    finished_beams.append(beam)
                    continue

                # Create masks
                tgt_padding_mask = model.create_padding_mask(tgt)
                tgt_look_ahead_mask = model.create_look_ahead_mask(tgt.size(1)).to(device)
                tgt_mask = tgt_padding_mask * tgt_look_ahead_mask

                # Decode
                decoder_output = model.decode(tgt, encoder_output, src_mask, tgt_mask)
                output = model.output_projection(decoder_output)

                # Get log probabilities
                log_probs = F.log_softmax(output[:, -1, :], dim=-1)

                # Get top-k tokens
                top_k_probs, top_k_indices = torch.topk(log_probs, beam_width)

                # Create new candidates
                for k in range(beam_width):
                    new_token = top_k_indices[0, k].unsqueeze(0).unsqueeze(0)
                    new_sequence = torch.cat([tgt, new_token], dim=1)
                    new_score = beam.score + top_k_probs[0, k].item()
                    new_length = beam.length + 1

                    candidates.append(BeamSearchNode(new_sequence, new_score, new_length))

            # Select top beams
            if candidates:
                beams = sorted(candidates, key=lambda x: x.get_avg_score(), reverse=True)[:beam_width]
            else:
                break

            # Stop if all beams finished
            if len(beams) == 0:
                break

        # Get best sequence from finished beams or active beams
        all_beams = finished_beams + beams
        if all_beams:
            best_beam = max(all_beams, key=lambda x: x.get_avg_score())
            return best_beam.sequence
        else:
            return initial_sequence


print("‚úì Beam search decoding function defined")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 6: Main Inference Function
# ----------------------------------------------------------------------------
print("=" * 80)
print("DEFINING MAIN INFERENCE FUNCTION")
print("=" * 80)

def generate_response(emotion, situation, customer_utterance, decoding_strategy='greedy', beam_width=5):
    """
    Generate empathetic response given context.

    Args:
        emotion: Emotion string (e.g., "happy", "sad")
        situation: Situation description
        customer_utterance: What the customer said
        decoding_strategy: 'greedy' or 'beam_search'
        beam_width: Beam width for beam search

    Returns:
        Dictionary with response and metadata
    """
    # Prepare input
    src = prepare_input(emotion, situation, customer_utterance, vocab)

    # Generate response
    max_len = config['suggested_max_target_len']

    if decoding_strategy == 'greedy':
        output, attention = greedy_decode(model, src, max_len, BOS_ID, EOS_ID, device)
    elif decoding_strategy == 'beam_search':
        output = beam_search_decode(model, src, max_len, BOS_ID, EOS_ID, device, beam_width)
        attention = None
    else:
        raise ValueError(f"Unknown decoding strategy: {decoding_strategy}")

    # Convert to text
    response_text = ids_to_text(output[0], id_to_token)

    return {
        'response': response_text,
        'emotion': emotion,
        'situation': situation,
        'customer': customer_utterance,
        'decoding_strategy': decoding_strategy
    }


print("‚úì Main inference function defined")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 7: Test Inference Functions
# ----------------------------------------------------------------------------
print("=" * 80)
print("TESTING INFERENCE FUNCTIONS")
print("=" * 80)

# Test with example
test_emotion = "happy"
test_situation = "I just got a promotion at work!"
test_customer = "I'm so excited about this new opportunity."

print("\nTest Input:")
print(f"  Emotion: {test_emotion}")
print(f"  Situation: {test_situation}")
print(f"  Customer: {test_customer}")

print("\nGenerating responses...")

# Greedy decoding
result_greedy = generate_response(
    test_emotion,
    test_situation,
    test_customer,
    decoding_strategy='greedy'
)

print(f"\nGreedy Decoding:")
print(f"  ‚Üí {result_greedy['response']}")

# Beam search
result_beam = generate_response(
    test_emotion,
    test_situation,
    test_customer,
    decoding_strategy='beam_search',
    beam_width=5
)

print(f"\nBeam Search (width=5):")
print(f"  ‚Üí {result_beam['response']}")

print("\n" + "=" * 80)
print("‚úÖ PART 1 COMPLETE - Inference functions ready!")
print("=" * 80)
print("\nNow run PART 2 to create the Gradio UI interface.")
print("=" * 80)

# ============================================================================
# CELL 6 - PART 2: TASK 5 - INFERENCE & UI (Gradio Interface)
# ============================================================================
# Empathetic Conversational Chatbot - Gradio UI
#
# This is PART 2 of Task 5, containing:
# - Gradio chatbot interface
# - Conversation history tracking
# - Decoding strategy selection
# - Interactive demo
#
# NOTE: Run PART 1 first before running this cell!
# ============================================================================

# ----------------------------------------------------------------------------
# STEP 1: Install and Import Gradio
# ----------------------------------------------------------------------------
print("=" * 80)
print("TASK 5 - PART 2: SETTING UP GRADIO UI")
print("=" * 80)

!pip install gradio -q

import gradio as gr
import datetime

print("‚úì Gradio installed and imported!")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 2: Conversation History Manager
# ----------------------------------------------------------------------------
print("=" * 80)
print("SETTING UP CONVERSATION MANAGER")
print("=" * 80)

class ConversationHistory:
    """Manage conversation history."""

    def __init__(self):
        self.history = []

    def add_exchange(self, customer, agent, emotion, situation):
        """Add a customer-agent exchange to history."""
        self.history.append({
            'timestamp': datetime.datetime.now().strftime("%H:%M:%S"),
            'customer': customer,
            'agent': agent,
            'emotion': emotion,
            'situation': situation
        })

    def get_formatted_history(self):
        """Get formatted conversation history."""
        if not self.history:
            return "No conversation history yet."

        formatted = []
        for i, exchange in enumerate(self.history, 1):
            formatted.append(f"**Turn {i}** ({exchange['timestamp']}) - Emotion: *{exchange['emotion']}*")
            formatted.append(f"üë§ **Customer:** {exchange['customer']}")
            formatted.append(f"ü§ñ **Agent:** {exchange['agent']}")
            formatted.append("---")

        return "\n".join(formatted)

    def clear(self):
        """Clear conversation history."""
        self.history = []


# Initialize conversation manager
conversation_manager = ConversationHistory()

print("‚úì Conversation manager initialized")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 3: Gradio Interface Functions
# ----------------------------------------------------------------------------
print("=" * 80)
print("DEFINING GRADIO INTERFACE FUNCTIONS")
print("=" * 80)

def chatbot_interface(customer_message, emotion, situation, decoding_strategy, beam_width, conversation_history):
    """
    Main chatbot interface function for Gradio.

    Args:
        customer_message: Customer's input message
        emotion: Selected emotion
        situation: Situation description
        decoding_strategy: 'Greedy' or 'Beam Search'
        beam_width: Beam width for beam search
        conversation_history: Current conversation history HTML

    Returns:
        Tuple of (agent_response, updated_conversation_history)
    """
    if not customer_message.strip():
        return "Please enter a message.", conversation_history

    # Map strategy name to function parameter
    strategy_map = {
        'Greedy': 'greedy',
        'Beam Search': 'beam_search'
    }

    # Generate response
    result = generate_response(
        emotion=emotion,
        situation=situation,
        customer_utterance=customer_message,
        decoding_strategy=strategy_map[decoding_strategy],
        beam_width=int(beam_width)
    )

    agent_response = result['response']

    # Add to conversation history
    conversation_manager.add_exchange(
        customer=customer_message,
        agent=agent_response,
        emotion=emotion,
        situation=situation
    )

    # Get updated history
    updated_history = conversation_manager.get_formatted_history()

    return agent_response, updated_history


def clear_conversation():
    """Clear conversation history."""
    conversation_manager.clear()
    return "", "Conversation history cleared."


print("‚úì Gradio functions defined")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 4: Create Gradio Interface
# ----------------------------------------------------------------------------
print("=" * 80)
print("CREATING GRADIO INTERFACE")
print("=" * 80)

# Define emotion options
emotion_options = [
    "happy", "sad", "angry", "anxious", "excited", "grateful",
    "surprised", "disappointed", "proud", "afraid", "annoyed",
    "caring", "confident", "content", "disgusted", "embarrassed",
    "faithful", "furious", "guilty", "hopeful", "impressed",
    "jealous", "joyful", "lonely", "nostalgic", "prepared",
    "sentimental", "terrified", "trusting", "devastated", "anticipating",
    "apprehensive", "ashamed", "neutral"
]

# Create Gradio interface with Blocks for more control
with gr.Blocks(title="Empathetic Chatbot", theme=gr.themes.Soft()) as demo:

    gr.Markdown(
        """
        # ü§ñ Empathetic Conversational Chatbot

        **Built with Transformer (Multi-Head Attention) from Scratch**

        This chatbot generates empathetic responses based on:
        - Your message
        - The emotional context
        - The situation you're in

        ---
        """
    )

    with gr.Row():
        with gr.Column(scale=2):
            gr.Markdown("### üí¨ Chat Interface")

            # Input fields
            customer_input = gr.Textbox(
                label="Your Message",
                placeholder="Type your message here...",
                lines=3
            )

            with gr.Row():
                emotion_input = gr.Dropdown(
                    choices=emotion_options,
                    label="Emotion",
                    value="happy",
                    info="Select the emotion that best describes the context"
                )

                situation_input = gr.Textbox(
                    label="Situation (Optional)",
                    placeholder="Describe the situation briefly...",
                    lines=2
                )

            # Decoding options
            gr.Markdown("### ‚öôÔ∏è Decoding Settings")

            with gr.Row():
                decoding_strategy = gr.Radio(
                    choices=["Greedy", "Beam Search"],
                    label="Decoding Strategy",
                    value="Greedy",
                    info="Greedy: Fast, Beam Search: Better quality"
                )

                beam_width = gr.Slider(
                    minimum=1,
                    maximum=10,
                    step=1,
                    value=5,
                    label="Beam Width",
                    info="Only used for Beam Search"
                )

            # Buttons
            with gr.Row():
                send_button = gr.Button("Send Message", variant="primary")
                clear_button = gr.Button("Clear History", variant="secondary")

            # Response output
            agent_response = gr.Textbox(
                label="ü§ñ Agent Response",
                lines=3,
                interactive=False
            )

        with gr.Column(scale=1):
            gr.Markdown("### üìú Conversation History")

            conversation_display = gr.Markdown(
                value="No conversation history yet.",
                label="History"
            )

    # Examples
    gr.Markdown("### üí° Try These Examples")

    gr.Examples(
        examples=[
            ["I just got a promotion at work!", "excited", "I've been working really hard for this", "Greedy", 5],
            ["I'm feeling really down today.", "sad", "Things haven't been going well lately", "Beam Search", 5],
            ["I can't believe this happened to me!", "surprised", "This was totally unexpected", "Greedy", 5],
            ["I'm worried about my exam tomorrow.", "anxious", "I haven't prepared as much as I should have", "Beam Search", 3],
            ["Thank you so much for your help!", "grateful", "You really made my day", "Greedy", 5],
        ],
        inputs=[customer_input, emotion_input, situation_input, decoding_strategy, beam_width],
        label="Click an example to try it"
    )

    # Footer
    gr.Markdown(
        """
        ---
        **Model Details:**
        - Architecture: Transformer Encoder-Decoder (built from scratch)
        - Training: 51,708 empathetic dialogues
        - Metrics: BLEU 97.85, ROUGE-L 0.15, chrF 74.04

        **Decoding Strategies:**
        - **Greedy**: Selects most probable token at each step (faster)
        - **Beam Search**: Explores multiple paths (better quality, slower)
        """
    )

    # Event handlers
    send_button.click(
        fn=chatbot_interface,
        inputs=[customer_input, emotion_input, situation_input, decoding_strategy, beam_width, conversation_display],
        outputs=[agent_response, conversation_display]
    )

    clear_button.click(
        fn=clear_conversation,
        inputs=[],
        outputs=[customer_input, conversation_display]
    )

print("‚úì Gradio interface created!")
print("=" * 80 + "\n")


# ----------------------------------------------------------------------------
# STEP 5: Launch Gradio Interface
# ----------------------------------------------------------------------------
print("=" * 80)
print("LAUNCHING GRADIO INTERFACE")
print("=" * 80)

# Launch the interface
demo.launch(
    share=True,  # Create public link
    debug=False,
    show_error=True
)

print("\n" + "=" * 80)
print("‚úÖ TASK 5 COMPLETE - GRADIO UI LAUNCHED!")
print("=" * 80)
print("\nüåê The interface is now running!")
print("üì± A public link has been generated for sharing")
print("=" * 80)

# ============================================================================
# DOWNLOAD ALL GENERATED FILES
# ============================================================================
# This cell packages and downloads all files generated during training
# ============================================================================

import os
import shutil
from datetime import datetime

print("=" * 80)
print("PACKAGING ALL GENERATED FILES FOR DOWNLOAD")
print("=" * 80)

# Create a downloads directory
download_dir = '/content/project_outputs'
os.makedirs(download_dir, exist_ok=True)

# List of files to download
files_to_download = {
    # Model files
    '/content/best_model.pt': 'model/best_model.pt',

    # Training outputs
    '/content/training_history.png': 'training/training_history.png',

    # Evaluation outputs
    '/content/evaluation_results.png': 'evaluation/evaluation_results.png',
    '/content/human_evaluation_samples.csv': 'evaluation/human_evaluation_samples.csv',

    # Preprocessed data
    '/content/preprocessed_data/vocab.json': 'preprocessed/vocab.json',
    '/content/preprocessed_data/id_to_token.json': 'preprocessed/id_to_token.json',
    '/content/preprocessed_data/config.json': 'preprocessed/config.json',
    '/content/preprocessed_data/train_df.csv': 'preprocessed/train_df.csv',
    '/content/preprocessed_data/val_df.csv': 'preprocessed/val_df.csv',
    '/content/preprocessed_data/test_df.csv': 'preprocessed/test_df.csv',
}

print("\nCopying files to download directory...\n")

copied_files = []
missing_files = []

for source, destination in files_to_download.items():
    dest_path = os.path.join(download_dir, destination)

    # Create subdirectories if needed
    os.makedirs(os.path.dirname(dest_path), exist_ok=True)

    if os.path.exists(source):
        shutil.copy2(source, dest_path)
        file_size = os.path.getsize(source) / (1024 * 1024)  # Convert to MB
        print(f"‚úì Copied: {destination} ({file_size:.2f} MB)")
        copied_files.append(destination)
    else:
        print(f"‚úó Missing: {source}")
        missing_files.append(source)

print("\n" + "=" * 80)
print(f"SUMMARY: {len(copied_files)} files copied, {len(missing_files)} missing")
print("=" * 80)

# Create a README file
readme_content = f"""# Empathetic Chatbot - Project Outputs
Generated on: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## Directory Structure:

### üìÅ model/
- best_model.pt - Best trained model (Epoch 17, BLEU: 14.25)

### üìÅ training/
- training_history.png - Training curves and metrics visualization

### üìÅ evaluation/
- evaluation_results.png - Test set evaluation metrics
- human_evaluation_samples.csv - Samples for human evaluation

### üìÅ preprocessed/
- vocab.json - Vocabulary (token -> id mapping)
- id_to_token.json - Reverse vocabulary (id -> token)
- config.json - Configuration and hyperparameters
- train_df.csv, val_df.csv, test_df.csv - Preprocessed datasets

## Model Performance:

### Training:
- Best Validation BLEU: 14.25 (Epoch 17)
- Final Training Loss: 3.46
- Validation Perplexity: 117.14

### Test Set:
- BLEU Score: 97.85
- ROUGE-L Score: 0.1522
- chrF Score: 74.04
- Perplexity: 48.92

## Model Architecture:
- Transformer Encoder-Decoder (from scratch)
- Embedding dimension: 512
- Attention heads: 8
- Encoder layers: 2
- Decoder layers: 2
- Total parameters: 31,589,457

## Training Details:
- Dataset: 64,636 empathetic dialogues
- Training samples: 51,708
- Validation samples: 6,463
- Test samples: 6,465
- Vocabulary size: 16,465
- Optimizer: Adam (betas=0.9, 0.98)
- Learning rate: 2e-4
- Batch size: 64

## Files Copied:
"""

for file in copied_files:
    readme_content += f"‚úì {file}\n"

if missing_files:
    readme_content += "\n## Missing Files:\n"
    for file in missing_files:
        readme_content += f"‚úó {file}\n"

readme_path = os.path.join(download_dir, 'README.md')
with open(readme_path, 'w') as f:
    f.write(readme_content)

print("\n‚úì README.md created")

# Create a zip file
print("\n" + "=" * 80)
print("CREATING ZIP ARCHIVE")
print("=" * 80)

zip_filename = '/content/empathetic_chatbot_outputs.zip'
shutil.make_archive(
    zip_filename.replace('.zip', ''),
    'zip',
    download_dir
)

zip_size = os.path.getsize(zip_filename) / (1024 * 1024)  # MB
print(f"\n‚úì Zip file created: empathetic_chatbot_outputs.zip ({zip_size:.2f} MB)")

# Download the zip file
print("\n" + "=" * 80)
print("DOWNLOADING ZIP FILE")
print("=" * 80)

from google.colab import files
files.download(zip_filename)

print("\n‚úì Download started! Check your browser's downloads folder.")
print("=" * 80)

# Also offer individual downloads
print("\n" + "=" * 80)
print("INDIVIDUAL FILE DOWNLOADS (Optional)")
print("=" * 80)
print("\nIf you want to download individual files, uncomment the lines below:\n")

print("""
# Download model
# files.download('/content/best_model.pt')

# Download training plot
# files.download('/content/training_history.png')

# Download evaluation plot
# files.download('/content/evaluation_results.png')

# Download human evaluation CSV
# files.download('/content/human_evaluation_samples.csv')

# Download config
# files.download('/content/preprocessed_data/config.json')
""")

print("=" * 80)
print("‚úÖ DOWNLOAD COMPLETE!")
print("=" * 80)
print("\nYou now have:")
print("  üì¶ empathetic_chatbot_outputs.zip")
print("  üìÑ Contains: model, plots, data, and README")
print("  üìä Total files: " + str(len(copied_files) + 1))  # +1 for README
print("=" * 80)